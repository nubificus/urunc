{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"urunc: A Lightweight Container Runtime for Unikernels","text":"<p>The main goal of <code>urunc</code> is to bridge the gap between traditional unikernels and containerized environments, enabling seamless integration with cloud-native architectures. Designed to fully leverage the container semantics and benefits from the OCI tools and methodology, <code>urunc</code> aims to become \u201crunc for unikernels\u201d, while offering compatibility with the Container Runtime Interface (CRI). Unikernels are packaged inside OCI-compatible images and <code>urunc</code> launches the unikernel on top of the underlying Virtual Machine or seccomp monitors. Thus, developers and administrators can package, deliver, deploy and manage unikernels using familiar cloud-native practises.</p> <p>For the above purpose <code>urunc</code> acts as any other OCI runtime. The main difference of <code>urunc</code> with other container runtimes is that instead of spawning a simple process, it uses a Virtual Machine Monitor (VMM) or a sandbox monitor to run the unikernel. It is important to note that <code>urunc</code> does not require any particular software running alongise the user's application inside or outside the unikernel. As a result, <code>urunc</code> is able to support any unikernel framework or similar technologies, while maintaining as low overhead as possible.</p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>OCI Compatibility: Compatible with the Open Container Initiative (OCI) standards, enabling the use of existing container tools and workflows.</li> <li>Container Runtime Interface (CRI) Support: Compatible with Kubernetes and other CRI-based systems for seamless integration into container orchestration platforms.</li> <li>Unikernel Support: Run applications and user code as unikernels, unlocking the performance and security advantages of unikernel technology.</li> <li>Integration with VMMs and other strong sandboxing mechanisms: Use lightweight VMMs or sandbox monitors to launch unikernels, facilitating efficient resource isolation and management.</li> <li>Un-opinionated and Extensible: Straightforward and easy integration of new unikernel frameworks and sandboxing mechanisms without any porting overhead.</li> </ul>"},{"location":"#use-cases","title":"Use cases","text":"<p>Unikernels are well known as a good fit for a variety of use cases, such as:</p> <ul> <li>Microservices: The lightweight and almost deminished OS noise of unikernels   can significantly improve the execution of applications, making unikernels an   attractive fit for microservices.</li> <li>Serverless and FaaS: The extremely fast instantiation time of unikernels   satisfies the event-driven, short-lived and scalable characteristics of   serverless computing</li> <li>Edge computing: The lightweight notion of unikernels suits very well with edge   devices, where resources constraints and performance are critical.</li> <li>Sensitive environments: The inherited strong VM-based isolation, along with   the minimized attack surface of unikernels, provide strong security guarantees   for sensitive applications which demand high security standards.</li> </ul> <p>In all the above use cases, <code>urunc</code> facilitates the seamless integration of unikernels with existing cloud-native tools and technologies, enabling the effortless distribution and management of applications running as unikernels.</p>"},{"location":"#current-support-of-unikernels-and-vmsandbox-monitors","title":"Current support of unikernels and VM/Sandbox monitors","text":"<p>The following table provides an overview of the currently supported VMMs and Sandbox monitors, along with the unikernels that can run on top of them.</p> Unikernel VM/Sandbox Monitor Arch Storage Rumprun Solo5-hvt, Solo5-spt x86, aarch64 Block/Devmapper Unikraft Qemu, Firecracker x86 Initrd MirageOS Qemu, Solo5-hvt, Solo5-spt x86, aarch64 Block/Devmapper Mewz Qemu x86 In-memory Linux Qemu, Firecracker x86, aarch64 Initrd, Block/Devmapper"},{"location":"#quick-links","title":"Quick links","text":"<ul> <li>Urunc Slack channel</li> <li>Contributing</li> <li>Getting metrics from <code>urunc</code></li> <li>Integration with k8s</li> </ul> <p> urunc is a Cloud Native Computing Foundation sandbox project. </p> <p> </p>"},{"location":"Sample-images/","title":"Sample Unikernel OCI images","text":"<p>In this document, you can find the images used to perform <code>urunc</code>'s end-to-end tests. This might be helpful for anyone looking to spawn some example unikernels using <code>urunc</code>.</p> <p>The naming convention used for these images is $APPLICATION-$HYPERVISOR-$UNIKERNEL-$ADDITIONAL_INFO:tag We plan to create and maintain multi-platform images soon, as well as enrich this list with new images.</p> <ul> <li>harbor.nbfc.io/nubificus/urunc/hello-hvt-rumprun-nonet:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-hvt-rumprun:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-hvt-mirage:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-spt-mirage:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-spt-rumprun-nonet:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-spt-rumprun:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-qemu-mewz:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-qemu-unikraft:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-world-qemu-linux-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-firecracker-unikraft:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-world-firecracker-linux-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-env-qemu-unikraft-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-env-qemu-linux-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-env-firecracker-unikraft-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-env-firecracker-linux-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-qemu-unikraft-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-qemu-linux:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-qemu-linux-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-hvt-rumprun-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-spt-rumprun-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-firecracker-unikraft-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-firecracker-linux:latest</li> <li>harbor.nbfc.io/nubificus/urunc/nginx-firecracker-linux-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/hello-server-qemu-mewz:latest</li> <li>harbor.nbfc.io/nubificus/urunc/httpreply-firecracker-unikraft:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-hvt-rumprun:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-spt-rumprun:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-hvt-rumprun-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-spt-rumprun-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-qemu-linux:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-qemu-unikraft-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-qemu-linux-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-firecracker-linux:latest</li> <li>harbor.nbfc.io/nubificus/urunc/redis-firecracker-linux-block:latest</li> <li>harbor.nbfc.io/nubificus/urunc/net-hvt-mirage:latest</li> <li>harbor.nbfc.io/nubificus/urunc/net-spt-mirage:latest</li> <li>harbor.nbfc.io/nubificus/urunc/net-qemu-mirage:latest</li> <li>harbor.nbfc.io/nubificus/urunc/block-test-hvt-mirage:latest</li> <li>harbor.nbfc.io/nubificus/urunc/block-test-spt-mirage:latest</li> <li>harbor.nbfc.io/nubificus/urunc/whoami-qemu-linux-initrd:latest</li> <li>harbor.nbfc.io/nubificus/urunc/whoami-firecracker-linux-initrd:latest</li> </ul>"},{"location":"hypervisor-support/","title":"Supported VMMs and software-based monitors","text":"<p>One of the main goals of <code>urunc</code> is to be a generic OCI unikernel runtime for various unikernel frameworks and similar technologies. In order to achieve that, we want to support as many Virtual Machine Monitors (VMMs) and other types of sandboxing mechanisms such as user-space monitors based on seccomp.</p> <p>In this document, we will go through the current state of <code>urunc</code>'s support for VMMs and monitors that utilize software-based isolation technologies. We will provide a brief description about them, along with installation instructions and a few comments regarding their integration with <code>urunc</code>.</p> <p>Note: In general, <code>urunc</code> expects all supported VM/Sandbox monitors to be available somewhere in the <code>$PATH</code>.</p>"},{"location":"hypervisor-support/#virtual-machine-monitors-vmms","title":"Virtual Machine Monitors (VMMs)","text":"<p>VMMs use hardware-assisted virtualization technologies in order to create a Virtual Machine (VM) where a guest OS will execute. It is one of the most widely used technology for providing strong isolation in multi-tenant environments. For the time being <code>urunc</code> supports 3 types of such VMMs: 1) Qemu, 2) Firecracker and 3) Solo5-hvt.</p>"},{"location":"hypervisor-support/#qemu","title":"Qemu","text":"<p>Qemu (Quick Emulator) is an open-source virtualization platform that enables the emulation of various hardware architectures. By leveraging Linux's KVM, Qemu is able to create VMs and manage their execution.  Some of the biggest advantages of Qemu are the mature and stable interface and codebase. In addition, Qemu supports various paravirtual devices, mostly based on VirtIO and allows the direct use of the host's devices with passthrough.</p>"},{"location":"hypervisor-support/#installing-qemu","title":"Installing Qemu","text":"<p>We can easily install Qemu through almost all package managers. For more details check Qemu's download page. For instance, in the case of Ubuntu, we can simply run the following command: <pre><code>sudo apt-get install qemu-system\n</code></pre></p>"},{"location":"hypervisor-support/#qemu-and-urunc","title":"Qemu and <code>urunc</code>","text":"<p>In the case of Qemu, <code>urunc</code> makes use of its <code>virtio-net</code> device to provide network support for the unikernel through a tap device. In addition, <code>urunc</code> can leverage Qemu's initrd option in order to provide the Unikernel with an initial RamFS (initramfs). However, Qemu supports various ways to provide storage in VMs such as block devices through virtio-blk, shared-fs through 9p and virtio-fs and initramfs.</p> <p>We plan to add support for all the above options, but as previously mentioned only Initramfs is supported for the time being.</p> <p>Supported unikernel frameworks with <code>urunc</code>:</p> <ul> <li>Unikraft</li> <li>MirageOS</li> <li>Mewz</li> <li>Linux</li> </ul> <p>An example unikernel:</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-qemu-unikraft-initrd:latest unikernel\n</code></pre>"},{"location":"hypervisor-support/#aws-firecracker","title":"AWS Firecracker","text":"<p>AWS Firecracker is an open-source virtualization technology developed by Amazon Web Services (AWS) that is designed to run serverless workloads efficiently. Firecracker provides a minimalist VMM, allowing the creation of lightweight virtual machines, called microVMs, that are faster and more resource-efficient than traditional VMs. In contrast with Qemu, Firecracker aims to provide a smaller set of devices for the VMs. The main benefit of Firecracker comes from its fast VM instantiation and guest OS boot.</p>"},{"location":"hypervisor-support/#installing-firecracker","title":"Installing Firecracker","text":"<p>Firecracker is not available through a package manger, but it can easily be installed. The Getting Started guide of Firecracker describes how users can set up Firecracker. Long story short, we can fetch a Firecracker binary with the following commands:</p> <pre><code>ARCH=\"$(uname -m)\" \nVERSION=\"v1.7.0\"\nrelease_url=\"https://github.com/firecracker-microvm/firecracker/releases\"\ncurl -L ${release_url}/download/${VERSION}/firecracker-${VERSION}-${ARCH}.tgz | tar -xz\n# Rename the binary to \"firecracker\"\nsudo mv release-${VERSION}-$(uname -m)/firecracker-${VERSION}-${ARCH} /usr/local/bin/firecracker\nrm -fr release-${VERSION}-$(uname -m)\n</code></pre> <p>It is important to note that <code>urunc</code> expects to find the <code>firecracker</code> binary located in the <code>$PATH</code> and named <code>firecracker</code>.</p> <p>Note: Since only Unikraft can boot on top of Firecracker (from the supported unikernels in <code>urunc</code>) we use the v1.7.0 version of Firecracker, due to some booting issues of Unikraft in newer versions.</p>"},{"location":"hypervisor-support/#firecracker-and-urunc","title":"Firecracker and <code>urunc</code>","text":"<p>In the case of Firecracker, <code>urunc</code> makes use of its <code>virtio-net</code> device to provide network support for the unikernel though a tap device. In addition, <code>urunc</code> can leverage Firecracker's initrd option in order to provide the Unikernel with an initial RamFS (initramfs). Firecracker does not support shared-fs between the host and the guest. However, it does provide support for virtio-block.</p> <p>We plan to add support for virtio-block, but as previously mentioned only Initramfs is supported for the time being.</p> <p>Supported unikernel frameworks with <code>urunc</code>:</p> <ul> <li>Unikraft</li> <li>Linux</li> </ul> <p>An example unikernel:</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-firecracker-unikraft-initrd:latest unikernel\n</code></pre>"},{"location":"hypervisor-support/#solo5-hvt","title":"Solo5-hvt","text":"<p>Solo5-hvt is a lightweight, high-performance VMM designed to run unikernels in a virtualized environment. As a part of the broader Solo5 project, Solo5-hvt provides a minimal, efficient abstraction layer for running unikernels on modern hardware, leveraging hardware virtualization technologies Some of the key benefits of Solo5-hvt is its simplicity and and extremely fast boot times of unikernels. In contrast to the other VMMs, Solo5-hvt does not provide support for virtIO devices. Instead, it defines its own interface, which can be used for network and block I/O.</p>"},{"location":"hypervisor-support/#installing-solo5-hvt","title":"Installing Solo5-hvt","text":"<p>Solo5 can be installed by building from source. However, in order to do that, we will need a few packages.</p> <pre><code>sudo apt install libseccomp-dev pkg-config build-essential\n</code></pre> <p>Next, we can clone and build <code>solo5-hvt</code>.</p> <pre><code>git clone -b v0.9.0 https://github.com/Solo5/solo5.git\ncd solo5\n./configure.sh &amp;&amp; make -j$(nproc)\n</code></pre> <p>It is important to note that <code>urunc</code> expects to find the <code>solo5-hvt</code> binary located in the <code>$PATH</code> and named as <code>solo5-hvt</code>. Therefore, to install it:</p> <pre><code>sudo cp tenders/hvt/solo5-hvt /usr/local/bin\n</code></pre>"},{"location":"hypervisor-support/#solo5-hvt-and-urunc","title":"Solo5-hvt and <code>urunc</code>","text":"<p>In the case of Solo5-hvt, <code>urunc</code> supports all the devices and utilizes a tap device to provide network in the unikernel. For the storage part, <code>urunc</code> supports the block storage interface of Solo5-hvt, which can be used in two ways, either with a block image inside the container image, or using the devmapper as a snapshotter.</p> <p>In the first case, we copy inside the container image a block image that contains all the data we want to pass in the unikernel.</p> <p>In the second case, we copy directly all the files we want the unikernel to access inside the container's image. Using devmapper <code>urunc</code> will use the container's image snapshot as a block image for the unikernel. It is important to note that the unikernel framework must support the respective filesystem type (e.g. ext2/3/4). This is the case for Rumprun unikernel.</p> <p>Supported unikernel frameworks with <code>urunc</code>:</p> <ul> <li>Rumprun</li> <li>MirageOS</li> </ul> <p>An example unikernel with a block image inside the conntainer's rootfs:</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-hvt-rumprun-block:latest unikernel\n</code></pre>"},{"location":"hypervisor-support/#software-based-isolation-monitors","title":"Software-based isolation monitors","text":"<p>Except for the traditional VM-based isolation solutions, there are other solutions which provide isolation using software-based technologies too. In that case the monitor interacts with a user-space kernel on top of which the application is running. The user-space kernel intercepts or defines a set of system calls and then forwards them to the monitor. To further strengthen security, it is common to use seccomp filters to limit the exposure of the host OS to the monitor.</p> <p>A well-known example of such a technology is gVisor. Unfortunately, gVisor does not support the execution of any unikernel framework.</p>"},{"location":"hypervisor-support/#solo5-spt","title":"Solo5-spt","text":"<p>In a similar way, Solo5-spt is a specialized backend for the Solo5 project, designed to run unikernels in systems that do not have access to hardware-assisted virtualization technologies. Solo5-spt executes a unikernel monitor with a seccomp filter allowing only seven system calls. The unikernel running on top of Solo5-spt interacts with this monitor through a similar interface with Solo5-hvt, facilitating network and block storage I/O. Solo5-spt can provide extremely fast intantiation times, very small overhead, along with performant execution.</p>"},{"location":"hypervisor-support/#installing-solo5-spt","title":"Installing Solo5-spt","text":"<p>The installation process of Solo5-spt is similar with the Solo5-hvt one. In fact, both projects share the same repository. Hence we can follow the same steps as in Solo5-hvt. At first, make sure to install the necessary packages.</p> <pre><code>sudo apt install libseccomp-dev pkg-config build-essential\n</code></pre> <p>Next, we can clone and build <code>solo5-spt</code>.</p> <pre><code>git clone -b v0.9.0 https://github.com/Solo5/solo5.git\ncd solo5\n./configure.sh &amp;&amp; make -j$(nproc)\n</code></pre> <p>It is important to note that <code>urunc</code> expects to find the <code>solo5-spt</code> binary located in the <code>$PATH</code> and named <code>solo5-spt</code>. Therefore, to install it:</p> <pre><code>sudo cp tenders/spt/solo5-spt /usr/local/bin\n</code></pre>"},{"location":"hypervisor-support/#solo5-spt-and-urunc","title":"Solo5-spt and <code>urunc</code>","text":"<p>Similarly with Solo5-hvt, <code>urunc</code> supports all the devices of Solo5-spt. For more information take a look at the respective Solo5-hvt section.</p> <p>Supported unikernel frameworks with <code>urunc</code>:</p> <ul> <li>Rumprun</li> <li>MirageOS</li> </ul> <p>An example unikernel which utilizes devmapper for block storage:</p> <pre><code>sudo nerdctl run --rm -ti --snapshotter devmapper --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-spt-rumprun:latest unikernel\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>This document guides you through the installation of <code>urunc</code> and all required components for executing all supported unikernels and VM/Sandbox monitors.</p> <p>We assume a vanilla ubuntu 22.04 environment, although <code>urunc</code> is able to run on a number of distros.</p> <p>We will be installing and setting up:</p> <ul> <li>git, wget, bc, make, build-essential</li> <li>runc</li> <li>containerd</li> <li>CNI plugins</li> <li>nerdctl</li> <li>devmapper</li> <li>Go 1.24.1</li> <li>urunc</li> <li>solo5-{hvt|spt}</li> <li>qemu</li> <li>firecracker</li> </ul> <p>Let's go.</p> <p>Note: Be aware that some instructions might override existing tools and services.</p>"},{"location":"installation/#install-required-dependencies","title":"Install required dependencies","text":"<p>The following packages are required to complete the installation. Depending on your specific needs, some of them may not be necessary in your use case.</p> <pre><code>sudo apt install git wget build-essential libseccomp-dev pkg-config\n</code></pre>"},{"location":"installation/#install-container-related-dependencies","title":"Install container-related dependencies","text":""},{"location":"installation/#install-runc-or-any-other-generic-container-runtime","title":"Install runc or any other generic container runtime","text":"<p><code>urunc</code> requires a typical container runtime (e.g. runc, crun) to handle any unsupported container images (for example, in k8s pods the pause container is delegated to <code>runc</code> and urunc handles only the unikernel container). In this guide we will use <code>runc</code>. You can build runc from source or download the latest binary following the commands:</p> <pre><code>RUNC_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/opencontainers/runc/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://github.com/opencontainers/runc/releases/download/v$RUNC_VERSION/runc.$(dpkg --print-architecture)\nsudo install -m 755 runc.$(dpkg --print-architecture) /usr/local/sbin/runc\nrm -f ./runc.$(dpkg --print-architecture)\n</code></pre>"},{"location":"installation/#install-containerd","title":"Install containerd","text":"<p>We will use containerd as a high-level runtime and its latest version. For alternative installation methods or other information, please check containerd's Getting Started guide.</p> <pre><code>CONTAINERD_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/containerd/containerd/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://github.com/containerd/containerd/releases/download/v$CONTAINERD_VERSION/containerd-$CONTAINERD_VERSION-linux-$(dpkg --print-architecture).tar.gz\nsudo tar Cxzvf /usr/local containerd-$CONTAINERD_VERSION-linux-$(dpkg --print-architecture).tar.gz\nrm -f containerd-$CONTAINERD_VERSION-linux-$(dpkg --print-architecture).tar.gz\n</code></pre>"},{"location":"installation/#install-containerd-service","title":"Install containerd service","text":"<p>To start containerd with systemd, we will need to setup the respective service.</p> <pre><code>CONTAINERD_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/containerd/containerd/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://raw.githubusercontent.com/containerd/containerd/v$CONTAINERD_VERSION/containerd.service\nsudo rm -f /lib/systemd/system/containerd.service\nsudo mv containerd.service /lib/systemd/system/containerd.service\nsudo systemctl daemon-reload\nsudo systemctl enable --now containerd\n</code></pre>"},{"location":"installation/#configure-containerd","title":"Configure containerd","text":"<p>We will generate the default containerd's configuration to build on top of it later.</p> <pre><code>sudo mkdir -p /etc/containerd/\nsudo mv /etc/containerd/config.toml /etc/containerd/config.toml.bak # There might be no existing configuration.\nsudo containerd config default | sudo tee /etc/containerd/config.toml\nsudo systemctl restart containerd\n</code></pre>"},{"location":"installation/#install-cni-plugins","title":"Install CNI plugins","text":"<p>To install the latest release of CNI plugins:</p> <pre><code>CNI_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/containernetworking/plugins/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://github.com/containernetworking/plugins/releases/download/v$CNI_VERSION/cni-plugins-linux-$(dpkg --print-architecture)-v$CNI_VERSION.tgz\nsudo mkdir -p /opt/cni/bin\nsudo tar Cxzvf /opt/cni/bin cni-plugins-linux-$(dpkg --print-architecture)-v$CNI_VERSION.tgz\nrm -f cni-plugins-linux-$(dpkg --print-architecture)-v$CNI_VERSION.tgz\n</code></pre>"},{"location":"installation/#install-nerdctl","title":"Install nerdctl","text":"<p>To install the latest release of <code>nerdctl</code>:</p> <pre><code>NERDCTL_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/containerd/nerdctl/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://github.com/containerd/nerdctl/releases/download/v$NERDCTL_VERSION/nerdctl-$NERDCTL_VERSION-linux-$(dpkg --print-architecture).tar.gz\nsudo tar Cxzvf /usr/local/bin nerdctl-$NERDCTL_VERSION-linux-$(dpkg --print-architecture).tar.gz\nrm -f nerdctl-$NERDCTL_VERSION-linux-$(dpkg --print-architecture).tar.gz\n</code></pre>"},{"location":"installation/#setup-thinpool-devmapper","title":"Setup thinpool devmapper","text":"<p>In order to make use of directly passing the container's snapshot as block device in the unikernel, we will need to setup the devmapper snapshotter. We can do that by first creating a thinpool, using the respective scripts in urunc's repo.</p> <pre><code>git clone https://github.com/urunc-dev/urunc.git\nsudo mkdir -p /usr/local/bin/scripts\nsudo mkdir -p /usr/local/lib/systemd/system/\nsudo cp urunc/script/dm_create.sh /usr/local/bin/scripts/dm_create.sh\nsudo cp urunc/script/dm_reload.sh /usr/local/bin/scripts/dm_reload.sh\nsudo chmod 755 /usr/local/bin/scripts/dm_create.sh\nsudo chmod 755 /usr/local/bin/scripts/dm_reload.sh\n</code></pre> <p>The above scripts create and reload respectively a thinpool that will be used for the devmapper snapshotter. Therefore, to create the thinpool, we can run:</p> <pre><code>sudo /usr/local/bin/scripts/dm_create.sh\n</code></pre> <p>However, when the system reboots, we will need to reload the thinpool with:</p> <pre><code>sudo /usr/local/bin/scripts/dm_reload.sh\n</code></pre>"},{"location":"installation/#create-a-service-for-thinpool-reloading","title":"Create a service for thinpool reloading","text":"<p>Alternatively, we can automatically reload the existing thinpool when a system reboots,by setting up a new service in systemd.</p> <pre><code>sudo cp urunc/script/dm_reload.service /usr/local/lib/systemd/system/dm_reload.service\nsudo chmod 644 /usr/local/lib/systemd/system/dm_reload.service\nsudo chown root:root /usr/local/lib/systemd/system/dm_reload.service\nsudo systemctl daemon-reload\nsudo systemctl enable dm_reload.service\n</code></pre>"},{"location":"installation/#configure-containerd-for-devmapper","title":"Configure containerd for devmapper","text":"<ul> <li>In containerd v2.x:</li> </ul> <pre><code>sudo sed -i \"/\\[plugins\\.'io\\.containerd\\.snapshotter\\.v1\\.devmapper'\\]/,/^$/d\" /etc/containerd/config.toml\nsudo tee -a /etc/containerd/config.toml &gt; /dev/null &lt;&lt;'EOT'\n\n# Customizations for devmapper\n\n[plugins.'io.containerd.snapshotter.v1.devmapper']\n  pool_name = \"containerd-pool\"\n  root_path = \"/var/lib/containerd/io.containerd.snapshotter.v1.devmapper\"\n  base_image_size = \"10GB\"\n  discard_blocks = true\n  fs_type = \"ext2\"\nEOT\nsudo systemctl restart containerd\n</code></pre> <ul> <li>In containerd v1.x:</li> </ul> <pre><code>sudo sed -i '/\\[plugins\\.\"io\\.containerd\\.snapshotter\\.v1\\.devmapper\"\\]/,/^$/d' /etc/containerd/config.toml\nsudo tee -a /etc/containerd/config.toml &gt; /dev/null &lt;&lt;'EOT'\n\n# Customizations for devmapper\n\n[plugins.\"io.containerd.snapshotter.v1.devmapper\"]\n  pool_name = \"containerd-pool\"\n  root_path = \"/var/lib/containerd/io.containerd.snapshotter.v1.devmapper\"\n  base_image_size = \"10GB\"\n  discard_blocks = true\n  fs_type = \"ext2\"\nEOT\nsudo systemctl restart containerd\n</code></pre> <p>Before proceeding, make sure that the new snapshotter is properly configured:</p> <pre><code>sudo ctr plugin ls | grep devmapper\nio.containerd.snapshotter.v1              devmapper                linux/amd64    ok\n</code></pre>"},{"location":"installation/#install-urunc","title":"Install urunc","text":""},{"location":"installation/#option-1-build-from-source","title":"Option 1: Build from source","text":""},{"location":"installation/#install-go","title":"Install Go","text":"<p>In order to build <code>urunc</code> from source, we need to install Go. Any version earlier than Go 1.20.6 will be sufficient.</p> <pre><code>GO_VERSION=1.24.1\nwget -q https://go.dev/dl/go${GO_VERSION}.linux-$(dpkg --print-architecture).tar.gz\nsudo mkdir /usr/local/go${GO_VERSION}\nsudo tar -C /usr/local/go${GO_VERSION} -xzf go${GO_VERSION}.linux-$(dpkg --print-architecture).tar.gz\nsudo tee -a /etc/profile &gt; /dev/null &lt;&lt; EOT\nport PATH=\\$PATH:/usr/local/go$GO_VERSION/go/bin\nT\nrm -f go${GO_VERSION}.linux-$(dpkg --print-architecture).tar.gz\n</code></pre> <p>Note: You might need to logout and log back in to the shell, in order to use Go.</p>"},{"location":"installation/#build-and-install-urunc","title":"Build and install urunc","text":"<p>After installing Go, we can clone and build <code>urunc</code>:</p> <pre><code>git clone https://github.com/urunc-dev/urunc.git\ncd urunc\nmake &amp;&amp; sudo make install\ncd ..\n</code></pre>"},{"location":"installation/#option-2-install-latest-release","title":"Option 2: Install latest release","text":"<p>We can also install <code>urunc</code> from its latest release:</p> <pre><code>URUNC_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/urunc-dev/urunc/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://github.com/urunc-dev/urunc/releases/download/v$URUNC_VERSION/urunc_$(dpkg --print-architecture)\nchmod +x urunc_$(dpkg --print-architecture)\nsudo mv urunc_$(dpkg --print-architecture) /usr/local/bin/urunc\n</code></pre> <p>And for <code>containerd-shim-urunc-v2</code>:</p> <pre><code>wget -q https://github.com/urunc-dev/urunc/releases/download/v$URUNC_VERSION/containerd-shim-urunc-v2_$(dpkg --print-architecture)\nchmod +x containerd-shim-urunc-v2_$(dpkg --print-architecture)\nsudo mv containerd-shim-urunc-v2_$(dpkg --print-architecture) /usr/local/bin/containerd-shim-urunc-v2\n</code></pre>"},{"location":"installation/#add-urunc-runtime-to-containerd","title":"Add urunc runtime to containerd","text":"<p>We also need to add <code>urunc</code> as a runtime in containerd's configuration:</p> <ul> <li>In containerd 2.x:</li> </ul> <pre><code>sudo tee -a /etc/containerd/config.toml &gt; /dev/null &lt;&lt;EOT\n[plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.urunc]\n    runtime_type = \"io.containerd.urunc.v2\"\n    container_annotations = [\"com.urunc.unikernel.*\"]\n    pod_annotations = [\"com.urunc.unikernel.*\"]\n    snapshotter = \"devmapper\"\nEOT\nsudo systemctl restart containerd\n</code></pre> <ul> <li>In containerd 1.x:</li> </ul> <pre><code>sudo tee -a /etc/containerd/config.toml &gt; /dev/null &lt;&lt;EOT\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.urunc]\n    runtime_type = \"io.containerd.urunc.v2\"\n    container_annotations = [\"com.urunc.unikernel.*\"]\n    pod_annotations = [\"com.urunc.unikernel.*\"]\n    snapshotter = \"devmapper\"\nEOT\nsudo systemctl restart containerd\n</code></pre>"},{"location":"installation/#install-qemu-firecracker-and-solo5","title":"Install Qemu, Firecracker and Solo5","text":""},{"location":"installation/#install-solo5","title":"Install Solo5","text":"<p>We can clone, build and install both <code>Solo5-hvt</code> and <code>Solo5-spt</code> from their common repository</p> <pre><code>git clone -b v0.9.0 https://github.com/Solo5/solo5.git\ncd solo5\n./configure.sh  &amp;&amp; make -j$(nproc)\nsudo cp tenders/hvt/solo5-hvt /usr/local/bin\nsudo cp tenders/spt/solo5-spt /usr/local/bin\n</code></pre>"},{"location":"installation/#install-qemu","title":"Install Qemu","text":"<p>Qemu installation can easily take place using the package manager.</p> <pre><code>sudo apt install qemu-system\n</code></pre>"},{"location":"installation/#install-firecracker","title":"Install Firecracker","text":"<p>To install firecracker, we will use the github release page of Firecracker. We choose to install version 1.7.0, since Unikraft has some issues with newer versions.</p> <pre><code>ARCH=\"$(uname -m)\"\nVERSION=\"v1.7.0\"\nrelease_url=\"https://github.com/firecracker-microvm/firecracker/releases\"\ncurl -L ${release_url}/download/${VERSION}/firecracker-${VERSION}-${ARCH}.tgz | tar -xz\n# Rename the binary to \"firecracker\"\nsudo mv release-${VERSION}-${ARCH}/firecracker-${VERSION}-${ARCH} /usr/local/bin/firecracker\nrm -fr release-${VERSION}-${ARCH}\n</code></pre>"},{"location":"installation/#run-example-unikernels","title":"Run example unikernels","text":"<p>Now, let's run some unikernels for every VM/Sandbox monitor, to make sure everything was installed correctly.</p>"},{"location":"installation/#run-a-redis-rumprun-unikernel-over-solo5-hvt","title":"Run a Redis Rumprun unikernel over Solo5-hvt","text":"<pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-hvt-rumprun-block:latest unikernel\n</code></pre>"},{"location":"installation/#run-a-redis-rumprun-unikernel-over-solo5-spt-with-devmapper","title":"Run a Redis rumprun unikernel over Solo5-spt with devmapper","text":"<pre><code>sudo nerdctl run --rm -ti --snapshotter devmapper --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-spt-rumprun:latest unikernel\n</code></pre>"},{"location":"installation/#run-a-nginx-unikraft-unikernel-over-qemu","title":"Run a Nginx Unikraft unikernel over Qemu","text":"<pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-qemu-unikraft-initrd:latest unikernel\n</code></pre>"},{"location":"installation/#run-a-nginx-unikraft-unikernel-over-firecracker","title":"Run a Nginx Unikraft unikernel over Firecracker","text":"<pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-firecracker-unikraft-initrd:latest unikernel\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This document acts as a quickstart guide to showcase <code>urunc</code> features. Please refer to the installation guide for more detailed installation instructions, or the design document for more details regarding <code>urunc</code>'s architecture.</p> <p>We can quickly set <code>urunc</code> either with docker or containerd and nerdctl. We assume a vanilla ubuntu 22.04 environment, although <code>urunc</code> is able to run on a number of GNU/Linux distributions.</p>"},{"location":"quickstart/#using-docker","title":"Using Docker","text":"<p>The easiest and fastest way to try out <code>urunc</code> would be with <code>docker</code> Before doing so, please make sure that the host system satisfies the following dependencies:</p> <ul> <li>Docker</li> <li>Qemu</li> <li><code>urunc</code> and <code>containerd-shim-urunc-v2</code> binaries.</li> </ul>"},{"location":"quickstart/#install-docker","title":"Install Docker","text":"<p>At first we need docker.</p> <pre><code>curl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\nrm get-docker.sh\nsudo groupadd docker # The group might already exist\nsudo usermod -aG docker $USER\n</code></pre> <p>Note: Please logout and log back in from the shell, in order to be able to use docker without sudo</p>"},{"location":"quickstart/#install-urunc-from-source","title":"Install <code>urunc</code> from source","text":"<p>Then we need <code>urunc</code>:</p> <pre><code>sudo apt install -y git make\ngit clone https://github.com/urunc-dev/urunc.git\ndocker run --rm -ti -v $PWD/urunc:/urunc -w /urunc golang:1.24 bash -c \"git config --global --add safe.directory /urunc &amp;&amp; make\"\nsudo make -C urunc install\n</code></pre>"},{"location":"quickstart/#a-docker-example","title":"A docker example","text":"<p>We will try out a Unikraft unikernel over Qemu.</p>"},{"location":"quickstart/#install-qemu","title":"Install Qemu","text":"<p>Let's make sure that Qemu is installed : <pre><code>sudo apt install -y qemu-system\n</code></pre></p>"},{"location":"quickstart/#run-the-unikernel","title":"Run the unikernel","text":"<p>Now we are ready to run Nginx as a Unikraft unikernel using docker and <code>urunc</code>:</p> <pre><code>$ docker run --rm -d --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-qemu-unikraft:latest unikernel\n67bec5ab9a748e35faf7c2079002177b9bdc806220e59b6b413836db1d6e4018\n</code></pre> <p>We can inspect the container and get its IP address:</p> <pre><code>$ docker inspect 67bec5ab9a748e35faf7c2079002177b9bdc806220e59b6b413836db1d6e4018 | grep IPAddress\n            \"SecondaryIPAddresses\": null,\n            \"IPAddress\": \"172.17.0.2\",\n                    \"IPAddress\": \"172.17.0.2\",\n</code></pre> <p>At last we can curl the Nginx server running inside Unikraft with:</p> <pre><code>$ curl 172.17.0.2\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Hello, world!&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;Hello, world!&lt;/h1&gt;\n  &lt;p&gt;Powered by &lt;a href=\"http://unikraft.org\"&gt;Unikraft&lt;/a&gt;.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"quickstart/#using-containerd-and-nerdctl","title":"Using containerd and nerdctl","text":"<p>The second way to quickly start with <code>urunc</code> would be by setting up a high-level container runtime (e.g. containerd) and using nerdctl.</p>"},{"location":"quickstart/#install-a-high-level-container-runtime","title":"Install a high-level container runtime","text":"<p>First step is to install containerd and setup basic functionality (the <code>CNI</code> plugins and a snapshotter).</p> <p>If a tool is already installed, skip to the next step.</p>"},{"location":"quickstart/#install-and-configure-containerd","title":"Install and configure containerd","text":"<p>We will install containerd from the package manager:</p> <pre><code>sudo apt install containerd\n</code></pre> <p>In this way we will also install <code>runc</code>, but not the necessary CNI plugins. However, before proceeding to CNI plugins, we will generate the default configuration for containerd.</p> <pre><code>sudo mkdir -p /etc/containerd/\nsudo mv /etc/containerd/config.toml /etc/containerd/config.toml.bak # There might be no configuration\nsudo containerd config default | sudo tee /etc/containerd/config.toml\nsudo systemctl restart containerd\n</code></pre>"},{"location":"quickstart/#install-cni-plugins","title":"Install CNI plugins","text":"<pre><code>CNI_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/containernetworking/plugins/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://github.com/containernetworking/plugins/releases/download/v$CNI_VERSION/cni-plugins-linux-$(dpkg --print-architecture)-v$CNI_VERSION.tgz\nsudo mkdir -p /opt/cni/bin\nsudo tar Cxzvf /opt/cni/bin cni-plugins-linux-$(dpkg --print-architecture)-v$CNI_VERSION.tgz\nrm -f cni-plugins-linux-$(dpkg --print-architecture)-v$CNI_VERSION.tgz\n</code></pre>"},{"location":"quickstart/#setup-thinpool-devmapper","title":"Setup thinpool devmapper","text":"<p>In order to make use of directly passing the container's snapshot as block device in the unikernel, we will need to setup the devmapper snapshotter. We can do that by first creating a thinpool, using the respective scripts in <code>urunc</code>'s repo</p> <pre><code>wget -q https://raw.githubusercontent.com/urunc-dev/urunc/refs/heads/main/script/dm_create.sh\nwget -q https://raw.githubusercontent.com/urunc-dev/urunc/refs/heads/main/script/dm_reload.sh\nsudo mkdir -p /usr/local/bin/scripts\nsudo mv dm_create.sh /usr/local/bin/scripts/dm_create.sh\nsudo mv dm_reload.sh /usr/local/bin/scripts/dm_reload.sh\nsudo chmod 755 /usr/local/bin/scripts/dm_create.sh\nsudo chmod 755 /usr/local/bin/scripts/dm_reload.sh\nsudo /usr/local/bin/scripts/dm_create.sh\n</code></pre> <p>Note: The above instructions will create the thinpool, but in case of reboot, you will need to reload it running the <code>dm_reload.sh</code> script. Otherwise check the installation guide for creating a service. </p> <p>At last, we need to modify containerd configuration for the new demapper snapshotter:</p> <ul> <li>In containerd v2.x:</li> </ul> <pre><code>sudo sed -i \"/\\[plugins\\.'io\\.containerd\\.snapshotter\\.v1\\.devmapper'\\]/,/^$/d\" /etc/containerd/config.toml\nsudo tee -a /etc/containerd/config.toml &gt; /dev/null &lt;&lt;'EOT'\n\n# Customizations for devmapper\n\n[plugins.'io.containerd.snapshotter.v1.devmapper']\n  pool_name = \"containerd-pool\"\n  root_path = \"/var/lib/containerd/io.containerd.snapshotter.v1.devmapper\"\n  base_image_size = \"10GB\"\n  discard_blocks = true\n  fs_type = \"ext2\"\nEOT\nsudo systemctl restart containerd\n</code></pre> <ul> <li>In containerd v1.x:</li> </ul> <pre><code>sudo sed -i '/\\[plugins\\.\"io\\.containerd\\.snapshotter\\.v1\\.devmapper\"\\]/,/^$/d' /etc/containerd/config.toml\nsudo tee -a /etc/containerd/config.toml &gt; /dev/null &lt;&lt;'EOT'\n\n# Customizations for devmapper\n\n[plugins.\"io.containerd.snapshotter.v1.devmapper\"]\n  pool_name = \"containerd-pool\"\n  root_path = \"/var/lib/containerd/io.containerd.snapshotter.v1.devmapper\"\n  base_image_size = \"10GB\"\n  discard_blocks = true\n  fs_type = \"ext2\"\nEOT\nsudo systemctl restart containerd\n</code></pre> <p>Let's verify that the new snapshotter is properly configured:</p> <pre><code>$ sudo ctr plugin ls | grep devmapper\nio.containerd.snapshotter.v1           devmapper                linux/amd64    ok\n</code></pre>"},{"location":"quickstart/#install-nerdctl","title":"Install nerdctl","text":"<p>After installing containerd a nifty tool like nerdctl is useful to get a realistic experience.</p> <pre><code>NERDCTL_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/containerd/nerdctl/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://github.com/containerd/nerdctl/releases/download/v$NERDCTL_VERSION/nerdctl-$NERDCTL_VERSION-linux-$(dpkg --print-architecture).tar.gz\nsudo tar Cxzvf /usr/local/bin nerdctl-$NERDCTL_VERSION-linux-$(dpkg --print-architecture).tar.gz\nrm -f nerdctl-$NERDCTL_VERSION-linux-$(dpkg --print-architecture).tar.gz\n</code></pre>"},{"location":"quickstart/#install-urunc-from-its-latest-release","title":"Install <code>urunc</code> from its latest release","text":"<p>At last, but not least, we will install <code>urunc</code> from its latest release. At first, we will install the <code>urunc</code> binary:</p> <pre><code>URUNC_VERSION=$(curl -L -s -o /dev/null -w '%{url_effective}' \"https://github.com/urunc-dev/urunc/releases/latest\" | grep -oP \"v\\d+\\.\\d+\\.\\d+\" | sed 's/v//')\nwget -q https://github.com/urunc-dev/urunc/releases/download/v$URUNC_VERSION/urunc_$(dpkg --print-architecture)\nchmod +x urunc_$(dpkg --print-architecture)\nsudo mv urunc_$(dpkg --print-architecture) /usr/local/bin/urunc\n</code></pre> <p>Secondly, we will install the <code>containerd-shim-urunc-v2</code> binary:.</p> <pre><code>wget -q https://github.com/urunc-dev/urunc/releases/download/v$URUNC_VERSION/containerd-shim-urunc-v2_$(dpkg --print-architecture)\nchmod +x containerd-shim-urunc-v2_$(dpkg --print-architecture)\nsudo mv containerd-shim-urunc-v2_$(dpkg --print-architecture) /usr/local/bin/containerd-shim-urunc-v2\n</code></pre>"},{"location":"quickstart/#a-nerdctl-containerd-example","title":"A nerdctl-containerd example","text":"<p>We will try out a Rumprun unikernel running over Solo5-hvt with nerdctl.</p>"},{"location":"quickstart/#install-solo5","title":"Install solo5","text":"<p>Lets install <code>solo5-hvt</code>:</p> <pre><code>sudo apt install make gcc pkg-config libseccomp-dev\ngit clone -b v0.9.0 https://github.com/Solo5/solo5.git\ncd solo5\n./configure.sh  &amp;&amp; make -j$(nproc)\nsudo cp tenders/hvt/solo5-hvt /usr/local/bin\n</code></pre>"},{"location":"quickstart/#run-the-unikernel_1","title":"Run the Unikernel!","text":"<p>Now, let's run a Redis unikernel on top of Rumprun and solo5-hvt:</p> <pre><code>sudo nerdctl run -d --snapshotter devmapper --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-hvt-rumprun:latest unikernel\n</code></pre> <p>We can inspect the running container to check it's IP address:</p> <pre><code>$ sudo nerdctl ps \nCONTAINER ID    IMAGE                                                      COMMAND        CREATED           STATUS    PORTS    NAMES\n8a415b278a9e    harbor.nbfc.io/nubificus/urunc/redis-hvt-rumprun:latest    \"unikernel\"    18 seconds ago    Up                 redis-hvt-rumprun-8a415\n$ sudo nerdctl inspect 8a415b278a9e | grep IPAddress\n            \"IPAddress\": \"10.4.0.2\",\n                    \"IPAddress\": \"10.4.0.2\",\n                    \"IPAddress\": \"172.16.1.2\",\n</code></pre> <p>and we can interact with the redis unikernel:</p> <pre><code>$ telnet 10.4.0.2 6379\nTrying 10.4.0.2...\nConnected to 10.4.0.2.\nEscape character is '^]'.\nping\n+PONG\nquit\n+OK\nConnection closed by foreign host.\n</code></pre>"},{"location":"unikernel-support/","title":"Unikernel support","text":"<p>Unikernels are specialized, minimalistic operating systems constructed to run a single application. By compiling only the necessary components of an OS into the final image, unikernels offer improved performance, security, and smaller footprints compared to traditional OS-based virtual machines.</p> <p>One of the main goals of <code>urunc</code> is to bridge the gap between unikernels and the cloud-native ecosystem. For that reason, <code>urunc</code> aims to support all the available unikernel frameworks and similar technologies.</p> <p>For the time being, <code>urunc</code> provides support for Unikraft and Rumprun unikernels.</p>"},{"location":"unikernel-support/#unikraft","title":"Unikraft","text":"<p>Unikraft is a POSIX-friendly and highly modular unikernel framework designed to make it easier to build optimized, lightweight, and high-performance unikernels. Unlike traditional monolithic unikernel approaches, Unikraft allows developers to include only the components necessary for their application, resulting in reduced footprint and improved performance. At the same time, Unikraft offers Linux binary compatibility allowing easier and effortless execution of existing applications on top of Unikraft.  With support for various programming languages and environments, Unikraft is ideal for building unikernels across a wide range of use cases.</p>"},{"location":"unikernel-support/#vmms-and-other-sandbox-monitors","title":"VMMs and other sandbox monitors","text":"<p>Unikraft can boot on top of both Xen and KVM hypervisors. Especially in the case of KVM, Unikraft supports Qemu and AWS Firecracker. In both cases, it gets network access through virtio-net. In the case of storage, to the best of our knowledge Unikraft supports two options: a) 9pFS sharing a directory between the host and the unikernel and b) initrd and therefore an initial RamFS.</p>"},{"location":"unikernel-support/#unikraft-and-urunc","title":"Unikraft and <code>urunc</code>","text":"<p>In the case of Unikraft, <code>urunc</code> supports both network and storage I/O over both Qemu and Firecracker VMMs. However, for the time being, <code>urunc</code> only offers support for the initrd option of Unikraft and not for shared-fs. On the other hand, the shared-fs option is Work-In-Progress and we will soon provide an update about this.</p> <p>Unikraft maintains a catalog with available applications as unikernel images. Check out our packaging page on how to get these images and run them on top of <code>urunc</code>.</p> <p>An example of Unikraft on top of Qemu with <code>urunc</code>:</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-qemu-unikraft-initrd:latest unikernel\n</code></pre> <p>Another example of Unikraft on top of Firecracker with <code>urunc</code>:</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-firecracker-unikraft-initrd:latest unikernel\n</code></pre>"},{"location":"unikernel-support/#mirage","title":"Mirage","text":"<p>MirageOS is a library operating system that constructs unikernels for secure, high-performance network applications across various cloud computing and mobile platforms. MirageOS uses the OCaml language, with libraries that provide networking, storage and concurrency support that work under Unix during development, but become operating system drivers when being compiled for production deployment. We can easily set up and build MirageOS unikernels with <code>mirage</code>, which can be installed throgu the Opam source package manager. The framework is fully event-driven, with no support for preemptive threading.</p> <p>MirageOS is characterized from the extremely fast start up times (just a few milliseconds), small binaries (usually a few megabytes), small footprint (requires a few megabytes of memory) and safe logic, as it is completely written in OCaml.</p>"},{"location":"unikernel-support/#vmms-and-other-sandbox-monitors_1","title":"VMMs and other sandbox monitors","text":"<p>MirageOS, as one of the first unikernel frameworks, provides support for a variety of hypervisors and platforms. In particular, MirageOS makes use of Solo5 and can execute as a VM over KVM/Xen and other OSes, such as BSD OSes (FreeBSD, OpenBSD) or even Muen. Especially for KVM, MirageOS supports Qemu and Solo5-hvt.  It can access the network through virtio-net in the case of Qemu and using Solo5's I/O interface in the case of Solo5. For storage, MirageOS supports block-based storage through virtio-block and Solo5's I/O in Qemu and Solo5 respectively.</p> <p>Furthermore, MirageOS is also possible to execute on top of Solo5-spt a sandbox monitor of Solo5 project that does not use hardware-assisted virtualization. In that context, MirageOS can access network and block storage through Solo5's I/O interface.</p>"},{"location":"unikernel-support/#mirageos-and-urunc","title":"MirageOS and <code>urunc</code>","text":"<p>In the case of MirageOS <code>urunc</code> provides support for Solo5, Solo5 and Qemu. For all monitors of Solo5 <code>urunc</code> allows the access of both network and block storage through Solo5's I/O interface and for Qemu through virtio-net and virtio-block.</p> <p>For the time being, the block image that the MirageOS unikernel access during its execution should be placed inside the container image.</p> <p>For more information on packaging MirageOS unikernels for <code>urunc</code> take a look at our packaging page.</p> <p>An example of MirageOS on top of Solo5 using a block image inside the container's rootfs with 'urunc':</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/net-mirage-hvt:latest unikernel\n</code></pre> <p>An example of MirageOS on top of Solo5 with 'urunc':</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/net-mirage-spt:latest unikernel\n</code></pre>"},{"location":"unikernel-support/#rumprun","title":"Rumprun","text":"<p>Rumprun is a unikernel framework based on NetBSD, providing support for a variety of POSIX-compliant applications. Rumprun is particularly useful for deploying existing POSIX applications with minimal modifications. As a consequence of its design Rumprun can be up-to-date with the latest changes of NetBSD. However, the current repositories are not totally up-to-date. The repository with the most recent NetBSD version is here.</p> <p>In addition, Rumprun maintains a repository with all ported applications that can be easily used on top of Rumprun.</p>"},{"location":"unikernel-support/#vmms-and-other-sandbox-monitors_2","title":"VMMs and other sandbox monitors","text":"<p>Rumprun, as one of the oldest unikernel frameworks, provides support for both Xen and KVM hypervisors. Especially in the case of KVM, Rumprun supports Qemu and Solo5-hvt. It can access the network through virtio-net in the case of Qemu and using Solo5's I/O interface in the case of Solo5.  As far as we concern, Rumprun only supports block storage through virtio-block and Solo5's I/O in Qemu and Solo5 respectively.</p> <p>Furthermore, Rumprun is also possible to execute on top of Solo5-spt a sandbox monitor of Solo5 project that does not use hardware-assisted virtualization. In that context, Rumprun can access network and block storage through Solo5's I/O interface.</p>"},{"location":"unikernel-support/#rumprun-and-urunc","title":"Rumprun and <code>urunc</code>","text":"<p>In the case of Rumprun, <code>urunc</code> provides support for Solo5 and Solo5, but not yet for Qemu. For all monitors of Solo5 <code>urunc</code> allows the access of both network and block storage through Solo5's I/O interface. In particular, <code>urunc</code> takes advantage of Rumprun block storage and ext2 filesystem support and allows the mounting of the containerd's snapshot directly in the unikernel. This is only possible using devmapper as a snapshotter in containerd. For more information on setting up devmapper, please take a look on our installation guide.</p> <p>Except for devmapper, <code>urunc</code> also supports the option of adding a block image inside the container image and attaching it to Rumprun.</p> <p>For more information on packaging Rumprun unikernels for <code>urunc</code> take a look at our packaging page.</p> <p>An example of Rumprun on top of Solo5 using a block image inside the container's rootfs with 'urunc':</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-hvt-rumprun-block:latest unikernel\n</code></pre> <p>An example of Rumprun on top of Solo5 using devmapper with 'urunc':</p> <pre><code>sudo nerdctl run --rm -ti --snapshotter devmapper --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-spt-rumprun:latest unikernel\n</code></pre>"},{"location":"unikernel-support/#mewz","title":"Mewz","text":"<p>Mewz is a unikernel framework written from scratch in Zig, targeting WASM workloads. In contrast to other WASM runtimes that execute on top of general purpose operating systems, Mewz is designed as a specialized kernel where WASM applications can execute. In this way, Mewz provides the minimal required features and environment for executing WASM workloads. In addition, every WASM application executes on a separate Mewz instance, maintaining the single-purpose notion of unikernels.</p> <p>According to the design of Mewz, the WASM application is transformed to an object file which is directly linked against the Mewz kernel. Therefore, when the Mewz kernel boots, it executes the linked WASM application. Mewz has partial support for WASI and it provides support for networking and an in-memory, read-only filesystem. In addition, Mewz has socket compatibility with WasmEdge,</p> <p>A few examples of Mewz unikernels can be found in the examples directory of Mewz's repository.</p>"},{"location":"unikernel-support/#vmms-and-other-sandbox-monitors_3","title":"VMMs and other sandbox monitors","text":"<p>Mewz can execute only on top of Qemu.  It can access the network through a virtio-net PCI device. In the case of storage, Mewz only supports an in-memory read-only filesystem, which is directly linked along with the kernel.</p>"},{"location":"unikernel-support/#mewz-and-urunc","title":"Mewz and <code>urunc</code>","text":"<p>In the case of Mewz, <code>urunc</code> provides support for Qemu. If the container is configured with network access, then <code>urunc</code> will use a virtio-net PCI device to provide network access to Mewz unikernels.</p> <p>For more information on packaging Mewz unikernels for <code>urunc</code> take a look at our packaging page.</p> <p>An example of Mewz on top of Qemu with 'urunc':</p> <pre><code>sudo nerdctl run -m 512M --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/hello-server-qemu-mewz:latest\n</code></pre> <p>Note: As far as we understand, Mewz requires at least 512M of memory to properly boot.</p>"},{"location":"unikernel-support/#linux","title":"Linux","text":"<p>Linux is maybe the most widely used kernel and the vast majority of servers in the cloud use an OS based on Linux kernel. As a result, most applications and services we run on the cloud are built targeting Linux. Of course, Linux is not a unikernel framework. However, thanks to its highly configurable build-system we can create very small, tailored Linux kernels for a single application. The concept was introduced by the Lupine project, which examined how we can turn the Linux kernel into a unikernel.</p> <p>Using Linux, we can execute the vast majority of the existing containers on top of <code>urunc</code>. However, the rational is to target single application containers and not fully-blown distro containers. Focusing on a single application, we can further minimize the Linux kernel and keep only the necessary components for a specific application. Such a design allows the creation of minimal and fast single-application kernels that we can execute on top of <code>urunc</code>.</p>"},{"location":"unikernel-support/#vmms-and-other-sandbox-monitors_4","title":"VMMs and other sandbox monitors","text":"<p>Linux has wide support for different hardware and virtualization targets. It can execute on top of Qemu and Firecracker. It can access the network and storage through various ways (e.g. paravirtualization, emulated devices etc.).</p>"},{"location":"unikernel-support/#linux-and-urunc","title":"Linux and <code>urunc</code>","text":"<p>Focusing on the single-application notion of using the Linux kernel, <code>urunc</code> provides support for both Qemu and Firecracker. For network, <code>urunc</code> will make use of virtio-net either through PCI or MMIO, depending on the monitor. In the case of storage, <code>urunc</code> uses virtio-block and initrd. In particular, <code>urunc</code> takes advantage of the extensive filesystem support of Linux and can directly mount containerd's snapshot directly to a Linux VM. This is only possible using devmapper as a snapshotter in containerd. For more information on setting up devmapper, please take a look on our installation guide.</p> <p>For more information on packaging applications and executing them on top of Linux with <code>urunc</code> take a look at our running existing containers tutorial.</p> <p>An example of a Nginx alpine image on top of Qemu and Linux with 'urunc' and devmapper as a snapshotter:</p> <pre><code>sudo nerdctl run --rm -ti --snapshotter devmapper --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/nginx-qemu-linux:latest\n</code></pre> <p>An example of a Redis alpine image transformed to a block file on top of Firecracker and Linux with 'urunc':</p> <pre><code>sudo nerdctl run --rm -ti --runtime io.containerd.urunc.v2 harbor.nbfc.io/nubificus/urunc/redis-firecracker-linux-block:latest\n</code></pre>"},{"location":"unikernel-support/#future-unikernels-and-frameworks","title":"Future unikernels and frameworks:","text":"<p>In the near future, we plan to add support for the following frameworks:</p> <p>OSv: An OS designed specifically to run as a single application on top of a hypervisor. OSv is known for its performance optimization and supports a wide range of programming languages, including Java, Node.js, and Python.</p>"},{"location":"design/","title":"Design","text":"<p>This section describes the high-level architecture of <code>urunc</code>, along with the design choices and limitations.</p>"},{"location":"design/#overview","title":"Overview","text":"<p><code>urunc</code> is a container runtime designed to bridge the gap between traditional unikernels and containerized environments. It enables seamless integration with cloud-native architectures by leveraging familiar OCI (Open Container Initiative) tools and methodologies. By acting as a unikernel container runtime compatible with the Container Runtime Interface (CRI), <code>urunc</code> allows unikernels to be managed like containers, opening up possibilities for lightweight, secure, and high-performance application deployment.</p> <p>In <code>urunc</code>, the user code runs inside a unikernel on top of a Virtual Machine Monitor (VMM) or a sandbox monitor. As a result, <code>urunc</code> guarantees strong isolation among the containers and inherits the enhanced security features of unikernels, such as their small attack surface.</p> <p>In the unikernel context a single-process application runs directly on top of a Virtual Machine (VM) or a sandbox. At the same time, in the VM context, every VM runs as a process. Subsequently, <code>urunc</code> combines these two characteristics and treats the VM's process, which executes the unikernel that runs the application, as the container's process. This way, <code>urunc</code> does not reuire any auxiliary process running alongside the unikernel, maintaining as less overhead as possible. Instead <code>urunc</code> directly manages the application running in the unikernel through the VMM or the sandbox monitor. Moreover, <code>urunc</code> does not require any modifications in the unikernel framework and hence all unikernel frameworks and similar technologies can easily integrate with <code>urunc</code>.</p>"},{"location":"design/#execution-flow","title":"Execution flow","text":"<p>The process of starting a new unikernel container with <code>urunc</code>, starts at the higher-level runtime (<code>containerd</code>) level:</p> <ul> <li><code>Containerd</code> unpacks the image into a supported snapshotter (e.g. <code>devmapper</code>)   and invokes <code>urunc</code>, as any other OCI runtime.</li> <li><code>urunc</code> parses the image's rootfs and annotations, initiating the required   setup procedures. In particular, it creates essential pipes for stdio, it   creates the container's state file and runs the <code>prestart</code> hooks (if any).</li> <li>Subsequently, <code>urunc</code> spawns a new process within a distinct network   namespace, stores its PID and invokes the <code>createRuntime</code> and   <code>createContainer</code> hooks.</li> <li>When <code>Containerd</code> starts the container <code>urunc</code> configures any required   resources such as block devices or  network interfaces and runs the   <code>statContainer</code> hooks.</li> <li>Depending on the specified unikernel type and annotations, <code>urunc</code> selects the   appropriate VMM or sandbox monitor (e.g.  Qemu, Solo5-spt) and boots the   unikernel. The unikernel runs inside its own isolated environment, interacting   with external systems through the namespaces and devices configured by   <code>urunc</code>.</li> <li>Finally the unikernel is up and running as a container, and we can manage its   lifecycle like any other container through <code>urunc</code> (e.g., stopping,   restarting, or deleting the container).</li> </ul>"},{"location":"design/#image-format-and-annotations","title":"Image Format and Annotations","text":"<p>To support unikernels in a containerized environment, <code>urunc</code> requires specific metadata embedded in OCI container images. These images must include the unikernel binary, configuration and any other files required from the application or the unikernel and the aforementioned metadata which dictate how the unikernel should be run. The metadata can be passed to <code>urunc</code> either in the form of annotations or as a specific file in the container's rootfs. For a detailed explanation and an up-to-date list of the currently supported annotations take alook at the packaging unikernels page.</p> <p>Although <code>urunc</code>-formatted unikernel images are not designed to be executed by other container runtimes, they can still be stored and distributed via generic container registries, such as Docker Hub or Harbor. This ensures compatibility with standard cloud-native workflows for building, shipping, and deploying applications.</p>"},{"location":"design/seccomp/","title":"Seccomp in Urunc","text":""},{"location":"design/seccomp/#overview","title":"Overview","text":"<p>Seccomp (Secure Computing Mode) is a Linux kernel security feature that restricts the system calls a process can make, limiting the kernel exposure to the processes. Container runtimes make use of this mechanism to further limit a container and enhance overall security. </p>"},{"location":"design/seccomp/#how-seccomp-is-used-in-urunc","title":"How Seccomp is used in 'urunc'","text":"<p>In 'urunc' the application does not execute directly in the host kernel. Instead, 'urunc' makes use of either a VMM (Virtual Machine Monitor) or the <code>solo5-spt</code> tender to execute the application inside a unikernel. As a result, in contrast with other container runtimes, in 'urunc' the applications do not share the same kernel.</p> <p>Thus, a malicious user must take control of the guest kernel and escape to the VMM before attacking the host. To further limit the exposure of the host kernel to the VMM, 'urunc' uses seccomp filters for each supported VMM. In particular, in the case of: - Firecracker, 'urunc' does not have to do anything more, since Firecracker by   default makes uses seccomp filters. - Qemu, 'urunc' makes use of Qemu's sandbox command line options to activate   all possible seccomp filters in Qemu. - Solo5-hvt, 'urunc' applies the seccomp filters before executing   'Solo5-hvt'. - Solo5-spt, 'urunc' can not do anything since solo5-spt makes use of seccomp by   itself.</p>"},{"location":"design/seccomp/#caveats-of-using-seccomp-in-urunc","title":"Caveats of using seccomp in 'urunc'","text":"<p>Since 'urunc', in most cases, makes use of the VMM's mechanisms to enforce the seccomp filters, 'urunc' heavily relies on the VMM to properly restrict the system calls the VMM can use.</p> <p>In the case of 'Solo5-hvt', since 'urunc' is responsible for applying the seccomp filters, proper identification of the required system calls is necessary. Unfortunately, due to dynamic linking and Go's runtime, it is impossible to always predict correctly for every system the necessary system calls for 'Solo5-hvt' execution.</p> <p>Nevertheless, 'Solo5-hvt' with seccomp in 'urunc' has been tested in Ubuntu 20.04 and Ubuntu 22.04. Using 'urunc' and solo5-hvt on different platforms might result in failed execution. For that reason, we strongly recommend running the seccomp test first, by <code>make test_nerdctl_Seccomp</code>. In case the test fails, the seccomp profile for 'Solo5-hvt' needs to get updated.</p> <p>For that reason, we created a toolset to identify the required system calls. The toolset, along with instructions on how to use it, can be found in goscall repository.</p>"},{"location":"design/seccomp/#setting-a-seccomp-profile","title":"Setting a seccomp profile","text":"<p>Due to its design, 'urunc' does not allow the definition of a seccomp profile other than the default. However, users can totally disable seccomp by using the <code>--security-opt seccomp=unconfined</code> command line option. In that scenario, 'urunc' will not make use of any seccomp filters in all the supported VMMs, except of 'Solo5-spt'.</p>"},{"location":"developer-guide/","title":"Developer Guide","text":"<p>In this section we provide useful information regarding the development of <code>urunc</code>. In particular, the section contains information about setting up a dev environment, checking performance traces and more.</p> <p>We kindly ask you to check the contributing page before submitting any pull requests, or opening new issues.</p>"},{"location":"developer-guide/Code-of-Conduct/","title":"Code of Conduct","text":"<p>All maintainers and communicty members of <code>urunc</code> must abide the CNCF Code of Conduct.</p>"},{"location":"developer-guide/contribute/","title":"Contributing","text":"<p>Urunc is an open-source project licenced under the Apache License 2.0. We welcome anyone who would be interested in contributing to <code>urunc</code>. As a first step, please take a look at the following document. The current document provides a high level overview of <code>urunc</code>'s code structure, along with a few guidelines regarding contributions to the project.</p>"},{"location":"developer-guide/contribute/#table-of-contents","title":"Table of contents:","text":"<ol> <li>Code organization</li> <li>How to contribute</li> <li>Opening an issue</li> <li>Requesting new features</li> <li>Submitting a PR</li> <li>Style guide</li> <li>Contact</li> </ol>"},{"location":"developer-guide/contribute/#code-organization","title":"Code organization","text":"<p>Urunc is written in Go and we structure the code and other files as follows:</p> <ul> <li><code>/</code>: The root directory contains the Makefile to build <code>urunc</code>, along with other non-code files, such as the licence, readme and more.</li> <li><code>/docs</code>: This directory contains all the documentation related to <code>urunc</code>, such as the installation guide, timestamping and more.</li> <li><code>/cmd</code>: This directory contains handlers for the various command line options of <code>urunc</code> and the implementation of containerd-shim.</li> <li><code>/internal/metrics</code>: This directory contains the implementation of the metrics logger, which is used for the internal measuring of <code>urunc</code>'s setup steps.</li> <li><code>/pkg</code>: This directory contains the majority of the code for <code>urunc</code>. In particular, the subdirectory <code>/pkg/network/</code> contains network related code as expected, while the <code>/pkg/unikontainers/</code> subdirectory contains the main logic of <code>urunc</code>, along with the VMM/unikernel related logic.</li> </ul> <p>Therefore, we expect any new documentation related files to be placed under <code>/docs</code> and any changes or new files in code to be either in the <code>/cmd/</code> or <code>/pkg/</code> directory.</p>"},{"location":"developer-guide/contribute/#how-to-contribute","title":"How to contribute","text":"<p>There are plenty of ways to contribute to an open source project, even without changing or touching the code. Therefore, anyone who is interested in this project is very welcome to contribute in one of the following ways:</p> <ol> <li>Using <code>urunc</code>. Try it out yourself and let us know your experience. Did everything work well? Were the instructions clear?</li> <li>Improve or suggest changes to the documentation of the project. Documentation is very important for every project, hence any ideas on how to improve the documentation to make it more clear are more than welcome.</li> <li>Request new features. Any proposals for improving or adding new features are very welcome.</li> <li>Find a bug and report it. Bugs are everywhere and some are hidden very well. As a result, we would really appreciate it if someone found a bug and reported it to the maintainers.</li> <li>Make changes to the code. Improve the code, add new functionalities and make <code>urunc</code> even more useful.</li> </ol>"},{"location":"developer-guide/contribute/#opening-an-issue","title":"Opening an issue","text":"<p>We use Github issues to track bugs and requests for new features. Anyone is welcome to open a new issue, which is either related to a bug or a request for a new feature.</p>"},{"location":"developer-guide/contribute/#reporting-bugs","title":"Reporting bugs","text":"<p>In order to report a bug or misbehavior in <code>urunc</code>, a user can open a new issue explaining the problem. For the time being, we do not use any strict template for reporting any issues. However, in order to easily identify and fix the problem, it would be very helpful to provide enough information. In that context, when opening a new issue regarding a bug, we kindly ask you to:</p> <ul> <li>Mark the issue with the bug label</li> <li> <p>Provide the following information:</p> <ol> <li>A short description of the bug.</li> <li>The respective logs both from the output and containerd.</li> <li>Urunc's version (either the commit's hash or the version).</li> <li>The CPU architecture, VMM and the Unikernel framework used.</li> <li>Any particular steps to reproduce the issue.</li> <li>Keep an eye on the issue for possible questions from the maintainers.</li> </ol> </li> </ul> <p>A template for an issue could be the following one: <pre><code>## Description\nAn explanation of the issue \n\n## System info\n\n- Urunc version:\n- Arch:\n- VMM:\n- Unikernel:\n\n## Steps to reproduce\nA list of steps that can reproduce the issue.\n</code></pre></p>"},{"location":"developer-guide/contribute/#requesting-new-features","title":"Requesting new features","text":"<p>We will be very happy to listen from users about new features that they would like to see in <code>urunc</code>. One way to communicate such a request is using Github issues. For the time being, we do not use any strict template for requesting new features. However, we kindly ask you to mark the issue with the enhancement label and provide a description of the new feature.</p>"},{"location":"developer-guide/contribute/#submitting-a-pr","title":"Submitting a PR","text":"<p>Anyone should feel free to submit a change or an addition to the codebase of <code>urunc</code>. Currently, we use Github's Pull Requests (PRs) to submit changes to <code>urunc</code>'s codebase. Before creating a new PR, please follow the guidelines below:</p> <ul> <li>Make sure that the changes do not break the building process of <code>urunc</code>.</li> <li>Make sure that all the tests run successfully.</li> <li>Make sure that no commit in a PR breaks the building process of <code>urunc</code></li> <li>Make sure to sign-off your commits.</li> <li>Provide meaningful commit messages, describing shortly the changes.</li> <li>Provide a meaningful PR message</li> </ul> <p>As soon as a new PR is created the following workflow will take place:</p> <ol> <li>The creator of the PR should invoke the tests by adding the <code>ok-to-test</code> label.</li> <li>If the tests pass, request from one or more <code>urunc</code>'s maintainers to review the PR.</li> <li>The reviewers submit their review.</li> <li>The author of the PR should address all the comments from the reviewers.</li> <li>As soon as a reviewer approves the PR, an action will add the appropriate git trailers in the PR's commits.</li> <li>The reviewer who accepted the changes will merge the new changes.</li> </ol>"},{"location":"developer-guide/contribute/#labels-for-the-ci","title":"Labels for the CI","text":"<p>We use github workflows to invoke some tests when a new PR opens for <code>urunc</code>. In particular, we perform the following workflows tests:</p> <ul> <li>Linting of the commit message. Please check the git commit message style below for more info.</li> <li>Spell check, since <code>urunc</code> repository contains its documentation too.</li> <li>License check</li> <li>Code linting for Go.</li> <li>Building artifacts for amd64 and aarch64.</li> <li>Unit tests</li> <li>End-to-end tests</li> </ul> <p>For a better control over the tests and workflows that run in a PR, we define three labels which can be used:</p> <ul> <li><code>ok-to-test</code>: Runs a full CI workflow, meaning all lint tests (commit   message, spellcheck, license), Go's linting, building for x86 and aarch64,   unit tests and at last end-to-end tests.</li> <li><code>skip-build</code>: Skips the building workflows along with unit and end-to end tests   running all the linting tests. This is useful when   the PR is related to docs and it can help for catching spelling errors etc. In   addition, if the changes are not related to the codebase, running the   end-to-end tests is not required and saves some time.</li> <li><code>skip-lint</code>: Skips the linting phase. This is particularly useful on draft   PRs, when we want to just test the functionality of the code (either a bug   fix, or a new feature) and defer the cleanup/polishing of commits, code, and   docs to when the PR will be ready for review.</li> </ul> <p>Note: Both <code>skip-build</code> and <code>skip-lint</code> assume that the <code>ok-to-test</code> label is added.</p>"},{"location":"developer-guide/contribute/#style-guide","title":"Style guide","text":""},{"location":"developer-guide/contribute/#git-commit-messages","title":"Git commit messages","text":"<p>Please follow the below guidelines for your commit messages:</p> <ul> <li>Limit the first line to 72 characters or less.</li> <li>Limit all the other lines to 80 characters</li> <li> <p>Follow the Conventional Commits   specification and, specifically, format the header as <code>&lt;type&gt;[optional scope]:   &lt;description&gt;</code>, where <code>description</code> must not end with a fullstop and <code>type</code>   can be one of:</p> </li> <li> <p>feat: A new feature</p> </li> <li>fix: A bug fix</li> <li>docs: Documentation only changes</li> <li>style: Changes that do not affect the meaning of the code (white-space,     formatting, missing semi-colons, etc)</li> <li>refactor: A code change that neither fixes a bug nor adds a feature</li> <li>perf: A code change that improves performance</li> <li>test: Adding missing tests</li> <li>build: Changes that affect the build system or external dependencies     (example scopes: gulp, broccoli, npm)</li> <li>ci: Changes to our CI configuration files and scripts (example scopes:     Travis, Circle, BrowserStack, SauceLabs)</li> <li>chore: Other changes that don't modify src or test files</li> <li>revert: Reverts a previous commit</li> <li>In case the PR is associated with an issue, please refer to it, using the git trailer <code>Fixes: #Nr_issue</code></li> <li>Always sign-off your commit message</li> </ul>"},{"location":"developer-guide/contribute/#golang-code-styde","title":"Golang code styde","text":"<p>We follow gofmt's rules on formatting GO code. Therefore, we ask all contributors to do the same. Go provides the <code>gofmt</code> tool, which can be used for formatting your code.</p>"},{"location":"developer-guide/contribute/#contact","title":"Contact","text":"<p>We kindly invite everyone interested in <code>urunc</code> to join our Slack channel. To directly communicate with the maintainers, feel free to drop an email At <code>urunc</code>'s maintainers' mailing list</p>"},{"location":"developer-guide/development/","title":"Setup a Dev environment","text":""},{"location":"developer-guide/development/#prerequisites","title":"Prerequisites","text":"<p>Most of the steps are covered in the installation document. Please refer to it for:</p> <ul> <li>installing a recent version of Go (e.g. 1.24)</li> <li>installing <code>containerd</code> and <code>runc</code></li> <li>setting up the devmapper snapshotter</li> <li>installing <code>nerdctl</code> and the <code>CNI</code> plugins</li> <li>installing the relevant hypervisors</li> </ul>"},{"location":"developer-guide/development/#crictl-installation","title":"crictl installation","text":"<p>In addition to the above, we strongly suggest to install crictl which <code>urunc</code> uses for its end-to-end tests. The following commands will install <code>crictl</code></p> <pre><code>VERSION=\"v1.30.0\" # check latest version in /releases page\nwget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz\nsudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin\nrm -f crictl-$VERSION-linux-amd64.tar.gz\n</code></pre> <p>Since default endpoints for <code>crictl</code> are now deprecated, we need to set them up:</p> <pre><code>sudo tee -a /etc/crictl.yaml &gt; /dev/null &lt;&lt;'EOT'\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 20\nEOT\n</code></pre>"},{"location":"developer-guide/development/#urunc-installation","title":"urunc installation","text":"<p>The next step is to clone and build <code>urunc</code>:</p> <pre><code>git clone https://github.com/urunc-dev/urunc.git\ncd urunc\nmake &amp;&amp; sudo make install\n</code></pre> <p>At last, please  validate that the dev environment has been set correctly by running the:</p> <ul> <li> <p>unit tests: <code>make unittest</code> and</p> </li> <li> <p>end-to-end tests: <code>sudo make e2etest</code></p> </li> </ul> <p>Note: When running <code>make</code> commands for <code>urunc</code> that will use go (i.e. build, unitest, e2etest) you might need to specify the path to the go binary with <code>sudo GO=$(which go) make</code>.</p>"},{"location":"developer-guide/development/#debugging","title":"Debugging","text":"<p>To enable debugging logs, we need to pass the <code>--debug</code> flag when calling <code>urunc</code>. Also, to facilitate easier debugging, when the <code>debug</code> flag is true all logs are propagated to the syslog.</p> <p>An easy way to achieve this is to create a Bash wrapper for <code>urunc</code>:</p> <pre><code>sudo mv /usr/local/bin/urunc /usr/local/bin/urunc.default\nsudo tee /usr/local/bin/urunc &gt; /dev/null &lt;&lt;'EOT'\n#!/usr/bin/env bash\nexec /usr/local/bin/urunc.default --debug \"$@\"\nEOT\nsudo chmod +x /usr/local/bin/urunc\n</code></pre>"},{"location":"developer-guide/maintainers/","title":"The current maintainers of <code>urunc</code>","text":"Name GitHub Username Email Responsibility Charalampos Mainas @cmainas cmainas@nubificus.co.uk Core Maintainer Georgios Ntoutsos @gntouts gntouts@nubificus.co.uk Core Maintainer Anastassios Nanos @ananos ananos@nubificus.co.uk Core Maintainer"},{"location":"developer-guide/security/","title":"Security Policy","text":"<p>Security is one of the main goals of <code>urunc</code>. If you discover a security issue, we ask that you report it promptly and privately to the maintainers.  This page provides information on when and how to report a vulnerability, along with the description of the process of handling such reports.</p>"},{"location":"developer-guide/security/#security-disclosure-policy","title":"Security disclosure policy","text":"<p>The <code>urunc</code> project follows a responsible disclosure model. All vulnerability reports are reviewed by the <code>urunc</code> maintainers.  If necessary, the report may be shared with other trusted contributors to aid in finding a proper fix. </p>"},{"location":"developer-guide/security/#reporting-vulnerabilities","title":"Reporting Vulnerabilities","text":"<p>Please do not open public issues or PRs to report a vulnerability. Instead, use the private vulnerability reporting of the <code>urunc</code>'s Github repository. In particular, in <code>urunc</code>'s repository page, navigate to the Security tab, click <code>Advisories</code> and then <code>Report a vulnerability</code>. Alternatively, the report can be filed via email at <code>security@urunc.io</code>. This address delivers your message securely to all maintainers.</p>"},{"location":"developer-guide/security/#vulnerability-handling-process","title":"Vulnerability handling process","text":"<p>Upon the receival of a vulnerability report, the following process will take place:</p> <ul> <li>the <code>urunc</code> maintainers will acknowledge and analyze the report within 48   hours</li> <li>A timeline (embargo period) will be agreed upon with the reporter(s) to keep   the vulnerability confidential until a fix is ready</li> <li>The maintainers will prioritize and begin addressing the issue. They may   request additional details or involve trusted contributors to help resolve   the problem securely</li> <li>Reporters are encouraged to participate in solution design or testing. The   maintainers will keep them updated throughout the process</li> <li>At the end of the timeline: a) a proper fix will be merged, b) a new (patched)   version of <code>urunc</code> will get released and c) a public advisory will get published,   giving credits to the reporter(s), unless they prefer to remain anonymous</li> </ul>"},{"location":"developer-guide/security/#what-to-include-in-the-report","title":"What to include in the report","text":"<p>To help the maintainers assess and resolve the issue efficiently, please use the following template:</p> <pre><code>## Title\n_Short title describing the problem._\n\n## Description\n\n### Summary\n_Short summary of the problem. Make the impact and severity as clear as possible. For example: Supplementary groups are not set up properly inside a container._\n\n### Details\n_Give all details on the vulnerability. Pointing to the incriminated source code is very helpful for the maintainer._\n\n### PoC\n_Complete instructions, including specific configuration details, to reproduce the vulnerability._\n\n### Impact\n_What kind of vulnerability is it? Who is impacted?_\n\n## Affected Products\n\n### Ecosystem\n_Should be something related to Go, C, Github Actions etc._\n\n### Package Name\n_eg. github.com/urunc-dev/urunc_\n\n### Affected Versions\n_eg. &lt; 0.5.0_\n\n### Patched Versions\n_eg. 0.5.1\n\n### Severity\n_eg. Low, Critical etc._\n</code></pre> <p>Also, please use one report per vulnerability and try to keep in touch in case the <code>urunc</code> maintainers require more information.</p>"},{"location":"developer-guide/security/#scope-clarification","title":"Scope clarification","text":"<p>As a sandboxed container runtime, <code>urunc</code> makes use of VM or software based monitors to spawn workloads. Therefore, before submitting a report, please ensure the issue lies within <code>urunc</code> itself and not in the guest (uni)kernel or the monitor. If the vulnerability is in those components, kindly report it to their respective teams. However, if urunc uses those components in a way that introduces a security issue, please report it to the urunc maintainers.</p>"},{"location":"developer-guide/timestamps/","title":"Execution time breakdown","text":"<p>To facilitate performance measurements, a few timestamps have been added to the code base to provide a clear view of the time spent on each part of the execution flow.</p>"},{"location":"developer-guide/timestamps/#timestamps","title":"Timestamps","text":"<p>The timestamps currently depicting each unikernel container execution are the following:</p> Timestamp ID Process Description TS00 create <code>urunc create</code> was invoked TS01 create unikontainer struct created for spec TS02 create initial setup completed TS03 create start reexec process (with or without pty) TS04 reexec <code>urunc create --reexec</code> was invoked TS05 reexec close nsenter pipes and setup base dir TS06 create received pids from nsenter TS07 create executed <code>CreateRuntime</code> hooks TS08 create sent <code>ACK</code> IPC message to <code>reexec</code> process TS09 reexec received <code>ACK</code> message from <code>create</code> TS10 create <code>urunc create</code> terminated TS11 start <code>urunc start</code> was invoked TS12 start unikontainer struct created from spec TS13 start sent <code>START</code> IPC message to <code>reexec</code> TS14 reexec received <code>START</code> message from <code>start</code> TS15 reexec joined sandbox network namespace TS16 reexec network setup completed TS17 reexec disk setup completed TS18 reexec <code>execve</code> the hypervisor process"},{"location":"developer-guide/timestamps/#timestamping-logging-method","title":"Timestamping logging method","text":"<p>To log the timestamps with minimal overhead, we opted to use the zerolog package. We were able to keep the delay caused by the timestamp logging in a low level, around 38351ns for the 20 timestamps required. In comparison, when using logrus the overhead was measured at around 71589ns.</p> <p>To run the benchmarks for the currently supported logging methods:</p> <pre><code>URUNC_TIMESTAMPS=1 GOFLAGS=\"-count=1\" go test ./tests/benchmarks -bench=. -count 5 -v\n</code></pre>"},{"location":"developer-guide/timestamps/#how-to-enable-timestamping","title":"How to enable timestamping","text":"<p>In order to capture the timestamps, a separate <code>containerd-shim</code> and container runtime must be configured in your system.</p> <p>To create the \"timestamping\" version of <code>containerd-shim-urunc-v2</code>:</p> <pre><code>sudo tee -a /usr/local/bin/containerd-shim-uruncts-v2 &gt; /dev/null &lt;&lt; 'EOT'\n#!/bin/bash\nURUNC_TIMESTAMPS=1 /usr/local/bin/containerd-shim-urunc-v2 $@\nEOT\nsudo chmod +x /usr/local/bin/containerd-shim-uruncts-v2\n</code></pre> <p>To add the \"timestamping\" urunc to containerd config:</p> <pre><code>sudo tee -a /etc/containerd/config.toml &gt; /dev/null &lt;&lt; 'EOT'\n# timestamping urunc\n[plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.uruncts]\n    runtime_type = \"io.containerd.uruncts.v2\"\n    container_annotations = [\"com.urunc.unikernel.*\"]\n    pod_annotations = [\"com.urunc.unikernel.*\"]\n    snapshotter = \"devmapper\"\nEOT\nsudo systemctl restart containerd.service\n</code></pre>"},{"location":"developer-guide/timestamps/#how-to-gather-timestamps","title":"How to gather timestamps","text":"<p>Now we need to run a unikernel using the new container runtime <code>uruncts</code>:</p> <pre><code>sudo nerdctl run --rm --snapshotter devmapper --runtime io.containerd.uruncts.v2 harbor.nbfc.io/nubificus/urunc/hello-hvt-rumprun:latest\n</code></pre> <p>The timestamp logs are located at <code>/tmp/urunc.zlog</code>:</p> <pre><code>$ cat /tmp/urunc.zlog | grep TS\n{\"containerID\":\"faaf830245ffab0df81927cebd7f11065e70c7703121fbc1b11d4bca49bab461\",\"timestampID\":\"cTS00\",\"time\":1703676366849599657}\n{\"containerID\":\"faaf830245ffab0df81927cebd7f11065e70c7703121fbc1b11d4bca49bab461\",\"timestampID\":\"cTS01\",\"time\":1703676366853466038}\n{\"containerID\":\"faaf830245ffab0df81927cebd7f11065e70c7703121fbc1b11d4bca49bab461\",\"timestampID\":\"TS00\",\"time\":1703676366853478852}\n{\"containerID\":\"faaf830245ffab0df81927cebd7f11065e70c7703121fbc1b11d4bca49bab461\",\"timestampID\":\"TS01\",\"time\":1703676366854590287}\n{\"containerID\":\"faaf830245ffab0df81927cebd7f11065e70c7703121fbc1b11d4bca49bab461\",\"timestampID\":\"TS02\",\"time\":1703676366854709857}\n# ... (rest of the output)\n</code></pre> <p>Note: the timestamp destination (<code>/tmp/urunc.zlog</code>) is hardcoded for the time being.</p>"},{"location":"developer-guide/timestamps/#gethering-the-timestamps","title":"Gethering the timestamps","text":"<p>There are 3 Python utilities inside the <code>script/performance</code> directory to help gather the timestamps.</p>"},{"location":"developer-guide/timestamps/#measure-single-container-execution","title":"Measure single container execution","text":"<p>To gather the timestamps produced by a single unikernel container execution, you can use the <code>measure_single.py</code> script, passing the desired container id.</p> <pre><code>cd urunc/script/performance\npython3 measure_single.py 15c769b9be14c59174626521f7964a8ae06e75c48c5cfd91e2829317c15d455b\n</code></pre> <p>If no container ID is specified, it will return an error:</p> <pre><code>$ python3 measure_single.py \nError: Container ID not specified!\n\nUsage:\n        measure_single.py &lt;CONTAINER_ID&gt;\n</code></pre> <p>Sample output:</p> <pre><code>$ python3 measure_single.py 1bd50216c1709b854f78d50ec36cbbc55e0d4bc2e1509344082b51edc974af6d\nTS00 -&gt; TS01:   1086512 ns\nTS01 -&gt; TS02:   97936 ns\nTS02 -&gt; TS03:   119786 ns\n# ... (rest of the output)\n</code></pre>"},{"location":"developer-guide/timestamps/#automatically-measure-multiple-containers","title":"Automatically measure multiple containers","text":"<p>To automatically gather the timestamps produced by multiple unikernel container executions you can use the <code>measure.py</code> script, passing the desired iterations amount. Make sure to use <code>sudo</code> or execute this script as root, as it relies on <code>nerdctl</code> for spawning the unikernel containers.</p> <pre><code>cd urunc/script/performance\nsudo python3 measure.py 5\n</code></pre> <p>If the amount of iterations is not specified, it will return an error:</p> <pre><code>$ sudo python3 measure.py \nError: Iterations not specified!\n\nUsage:\n        measure.py &lt;ITERATIONS&gt;\n</code></pre> <p>Sample output:</p> <pre><code>$ sudo python3 measure.py 2\n{'TS00 -&gt; TS01': {'average': '11544405 ns',\n                  'maximum': '22292698 ns',\n                  'minimum': '796112 ns'},\n 'TS01 -&gt; TS02': {'average': '127228 ns',\n                  'maximum': '157051 ns',\n                  'minimum': '97405 ns'},\n 'TS02 -&gt; TS03': {'average': '120198 ns',\n                  'maximum': '162634 ns',\n                  'minimum': '77763 ns'},\n# ... (rest of the output)\n</code></pre> <p>The same functionality is provided by <code>measure_to_json.py</code>, but instead of <code>stdout</code> the results are saved in a .json file:</p> <pre><code>$ sudo python3 measure_to_json.py 5 ts.json\n$ cat ts.json | jq\n{\n  \"TS00 -&gt; TS01\": {\n    \"maximum\": \"989525 ns\",\n    \"minimum\": \"474103 ns\",\n    \"average\": \"719644 ns\"\n  },\n  \"TS01 -&gt; TS02\": {\n    \"maximum\": \"212337 ns\",\n    \"minimum\": \"76951 ns\",\n    \"average\": \"122868 ns\"\n# ... (rest of the output)\n</code></pre> <p>If the amount of iterations or output file are not specified, it will return an error:</p> <pre><code>$ sudo python3 measure_to_json.py 5 \nError: Iterations or output file not specified!\n\nUsage:\n        measure_to_json.py &lt;ITERATIONS&gt; &lt;OUTPUT&gt;\n</code></pre>"},{"location":"package/","title":"Building/Packaging unikernels","text":"<p>The OCI (Open Container Initiative) image format is a standardized specification for packaging and distributing containerized applications across different platforms and container runtimes. It defines a common structure for container images, including their metadata, layers, and filesystem content.</p> <p>Since <code>urunc</code> is an OCI-compatible container runtime, it expects the unikernel to be placed inside an OCI container image. Nevertheless, in order to differentiate between traditional container images and unikernel OCI images, <code>urunc</code> makes use of annotations or a metadata file (<code>urunc.json</code>) inside the container's rootfs.</p> <p>To facilitate the process, we provide various tools that build and package a unikernel binary, along with the application's necessary files in a container image and set the respective annotations. In particular, we can produce an OCI image with all <code>urunc</code>'s annotations using:</p> <ol> <li>bunny a tool that builds and packages unikernels     using buildkit's LLB and can also act as a frontend for    buildkit.</li> <li>bunix which uses Nix    packages to package a unikernel as an OCI image.</li> </ol> <p>In this section, we will first explain all the annotations that <code>urunc</code> expects, in order to handle unikernels and describe how to build and package unikernels as OCI images using the aforementioned tools.</p> <p>Quick links:</p> <ul> <li>Packaging pre-built unikernels</li> <li>Using unikernels from existing OCI images</li> <li>Packaging and creating unikernel's rootfs</li> </ul>"},{"location":"package/#annotations","title":"Annotations","text":"<p>OCI annotations are key-value metadata used to describe and provide additional context for container images and runtime configurations within the OCI specification. Using these annotations developers can embed non-essential information about containers, such as version details, licensing, build information, or custom runtime parameters, without affecting the core functionality of the container itself. The annotations can be placed in several components of the specification. However, in the case of <code>urunc</code> we are interested about annotations which can reach the container runtime.</p> <p>Using these annotations <code>urunc</code> receives information regarding the type of the unikernel, the VMM or sandbox mechanism to use and more. For the time being, the required annotations are the following:</p> <ul> <li><code>com.urunc.unikernel.unikernelType</code>: The type of the unikernel. Currently   supported values: a) unikraft, b) rumprun, c) mirage.</li> <li><code>com.urunc.unikernel.hypervisor</code>: The VMM or sandbox monitor to run the   unikernel Currently supported values: a) <code>qemu</code>, b) <code>firecracker</code>, c) <code>spt</code>,   d) <code>hvt</code>.</li> <li><code>com.urunc.unikernel.binary</code>: The path to the unikernel binary inside the   container's rootfs</li> <li><code>com.urunc.unikernel.cmdline</code>: The application's cmdline to pass to the   unikernel.</li> </ul> <p>Except of the above, <code>urunc</code> accepts the following optional annotations:</p> <ul> <li><code>com.urunc.unikernel.initrd</code>: The path to the initrd of the unikernel inside   the container's rootfs.</li> <li><code>com.urunc.unikernel.unikernelVersion</code>: The version of the unikernel framework (e.g.   0.17.0).</li> <li><code>com.urunc.unikernel.block</code>: The path to a block image inside container's   rootfs, which will get attached to the unikernel.</li> <li><code>com.urunc.unikernel.blkMntPoint</code>: The mount point of the block image to   attach in the unikernel.</li> <li><code>com.urunc.unikernel.useDMBlock</code>: A boolean value that if it is <code>true</code>, requests   from <code>urunc</code> to mount the container's image rootfs in the unikernel, Requires   the <code>devmapper</code> snapshotter.</li> </ul> <p>Due to the fact that Docker and some high-level container runtimes do not pass the image annotations to the underlying container runtime, <code>urunc</code> can also read the above information from a file inside the container's rootfs. The file should be named <code>urunc.json</code>, it should be placed in the root directory of the container's rootfs and it should have a JSON format with the above information, where the values are base64 encoded.</p>"},{"location":"package/#tools-to-construct-oci-images-with-uruncs-annotations","title":"Tools to construct OCI images with <code>urunc</code>'s annotations","text":"<p>As previously mentioned we currently provide 2 different tools to build and package unikernels in OCI images with <code>urunc</code>'s annotations.</p>"},{"location":"package/#bunny","title":"bunny","text":"<p>In an effort to simplify the process of building various unikernels, we built bunny. Except of building unikernels bunny can also pack existing unikernels (whether locally or from OCI images) as OCI images for <code>urunc</code>. At its core bunny leverages buildkit's LLB, allowing us to create OCI images from any type of file. Currently bunny can process two formats of files: a) the typical Dockerfile-like syntax files and b) <code>bunnyfile</code>, a specialized YAML-based file.</p> <p>When using Dockerfile-like files, bunny can only package pre-built unikernel images; it cannot build them. This format is primarily retained for compatibility with pun and bima, which are no longer maintained. Currently, bunny can handle the following instructions:</p> <ul> <li><code>FROM</code>: Specify an existing OCI image to use as a base.</li> <li><code>COPY</code>: this works as in Dockerfiles. At this moment, only a single copy   operation per instruction (think one copy per line). These files are copied   inside the container's image rootfs.</li> <li><code>LABEL</code>: all LABEL instructions are added as annotations to the container's   image. They are also added to a special <code>urunc.json</code> inside the container's image   rootfs.</li> </ul> <p>To further extend the functionality and provide a common interface to facilitate unikernel building, we defined <code>bunnyfile</code>. It is a YAML-based special file that bunny transforms to LLB with all the necessary steps to build the respective unikernel. Except of building unikernels, bunny can also be used to build or append files in the unikernel's rootfs.</p> <p>The current syntax of <code>bunnyfile</code> is the following one:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest   # [1] Set bunnyfile syntax for automatic recognition from buildkit.\nversion: v0.1                                   # [2] Bunnyfile version.\n\nplatforms:                                      # [3] The target platform for building/packaging.\n  framework: unikraft                           # [3a] The unikernel framework.\n  version: v0.15.0                              # [3b] The version of the unikernel framework.\n  monitor: qemu                                 # [3c] The hypervisor/VMM or any other kind of monitor.\n  architecture: x86                             # [3d] The target architecture.\n\nrootfs:                                         # [4] (Optional) Specifies the rootfs of the unikernel.\n  from: local                                   # [4a] (Optional) The source of the rootfs.\n  path: initrd                                  # [4b] (Required if from is not scratch) The path in the source, where the prebuilt rootfs file resides.\n  type: initrd                                  # [4c] (optional) The type of rootfs (e.g. initrd, raw, block)\n  include:                                      # [4d] (Optional) A list of local files to include in the rootfs\n    - src:dst\n\nkernel:                                         # [5] Specify a prebuilt kernel to use\n  from: local                                   # [5a] Specify the source of a prebuilt kernel.\n  path: kernel                                  # [5b] The path where the kernel image resides.\n\ncmdline: hello                                  # [6] The cmdline of the app.\n</code></pre> <p>For more information regarding the <code>bunnyfile</code> please take a look at the respective section of bunny's README. Furthermore, you can find various different examples and use cases for bunny in the examples directory of bunny's repository.</p>"},{"location":"package/#packaging-a-unikernel-with-bunny","title":"Packaging a unikernel with bunny","text":"<p>Since bunny uses buildkit it supports two modes of execution. In the first mode it acts as a buildkit frontend and in the second mode it outputs a LLB which can be passed to <code>buildctl</code>.Therefore, bunny depends on buildkit which should be installed. However, if docker is already installed, the frontend execution mode of bunny can be used directly without building or installing anything.</p> <p>It is important to note that if we want to use bunny as a frontend for buildkit we need to start the Containerfile with the following line:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:&lt;version&gt;\n</code></pre> <p>Using a Dockerfile-like syntax file</p> <p>If we want to package a locally built Nginx Unikraft unikernel, we can define the a Dockerfile-like syntax file as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nFROM scratch\n\nCOPY nginx-qemu-x86_64-initrd_qemu-x86_64 /unikernel/kernel\nCOPY rootfs.cpio /unikernel/initrd\n\nLABEL \"com.urunc.unikernel.binary\"=/unikernel/kernel\nLABEL \"com.urunc.unikernel.initrd\"=/unikernel/initrd\nLABEL \"com.urunc.unikernel.cmdline\"=\"nginx -c /nginx/conf/nginx.conf\"\nLABEL \"com.urunc.unikernel.unikernelType\"=\"unikraft\"\nLABEL \"com.urunc.unikernel.hypervisor\"=\"qemu\"\n</code></pre> <p>Using bunnyfile</p> <p>If we want to package the same unikernel, using <code>bunnyfile</code>, we have to define the file as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: unikraft\n  monitor: qemu\n  architecture: x86\n\nrootfs:\n  from: local\n  path: rootfs.cpio\n\nkernel:\n  from: local\n  path: nginx-qemu-x86_64-initrd_qemu-x86_64\n\ncmdline: nginx -c /nginx/conf/nginx.conf\n</code></pre> <p>and we can build it with a docker command:</p> <pre><code>docker build -f bunnyfile -t nubificus/urunc/nginx-unikraft-qemu:test .\n</code></pre> <p>NOTE: We can use the above command and switch form bunnyfile to the Dockerfile-like file and build the same unikernel OCI image.</p> <p>For more information check bunny's README.</p>"},{"location":"package/#bunix","title":"bunix","text":"<p>For Nix users, we have created a set of Nix scripts that we maintain in the bunix repository to build container images for <code>urunc</code>. In contrast to the previous tools, bunix uses a nix file to define the files to package as a container image, along with the <code>urunc</code> annotations. In particular, this file is the <code>args.nix</code> file, which expects the same fields:</p> <ul> <li>name: the name of the container image that Nix will build</li> <li>tag: the tag of the container image that Nix will build</li> <li>files: a list of key-value pairs with all the files to copy inside the   container image. The key-value pairs have the following format:   <code>\"&lt;path-based-on-cwd&gt;\" = \"&lt;path-inside-container&gt;\"</code>.</li> <li>annotations: a list will all the <code>urunc</code> annotations.</li> </ul>"},{"location":"package/#packaging-a-unikernel-with-bunix","title":"Packaging a unikernel with bunix","text":"<p>A necessary requirement to use bunix is the presence of Nix package manager. Then using bunix is as simple as completing the <code>args.nix</code> file.</p> <p>For example to package a locally built Rumprun Hello world unikernel running on top of Solo5-hvt, we should set the <code>args.nix</code> file as:</p> <pre><code>{\n  name = \"nginx-unikraft-qemu\";\n  tag = \"test\";\n  files = {\n    \"./nginx-qemu-x86_64-initrd_qemu-x86_64\" = \"/unikernel/kernel\";\n    \"./rootfs.cpio\" = \"/unikernel/initrd\";\n  };\n  annotations = {\n    unikernelType = \"unikraft\";\n    hypervisor = \"qemu\";\n    binary = \"/unikernel/kernel\";\n    cmdline = \"nginx -c /nginx/conf/nginx.conf\";\n    unikernelVersion = \"\";\n    initrd = \"/unikernel/initrd\";\n    block = \"\";\n    blkMntPoint = \"\";\n  };\n}\n</code></pre> <p>Then we can build the image by simply running the following command inside the repository:</p> <pre><code>nix-build default.nix\n</code></pre> <p>The above command will create a container image in a tar inside Nix's store. For easier access of the tar, Nix creates a symlink of the tar file in the CWD. The symlink will be named as <code>result</code>. Therefore, we can load the container image with:</p> <pre><code>docker load &lt; result\n</code></pre> <p>Please check bunix's README for more information.</p>"},{"location":"package/pre-built/","title":"Packaging pre-built unikernels for <code>urunc</code>","text":"<p>In this page we will explain the process of packaging an existing / pre-built unikernel as an OCI image with the necessary annotations for <code>urunc</code>. As an example, we will use a network example over MirageOS from mirage-skeleton targeting Solo5-hvt.</p> <p>For simply packaging pre-built unikernel images, we can use both bunny and bunix.</p> <p>NOTE: The below steps can be easily adjusted to any pre-built unikernel image.</p>"},{"location":"package/pre-built/#using-bunny","title":"Using <code>bunny</code>","text":"<p>In the case of bunny and pre-built unikernel images, we can use both supported file syntaxes: a) <code>bunnyfile</code> and b) the Dockerfile-like syntax.</p>"},{"location":"package/pre-built/#using-a-bunnyfile","title":"Using a <code>bunnyfile</code>","text":"<p>In order to package an existing pre-built unikernel image with bunny and a <code>bunnyfile</code> we can define the <code>bunnyfile</code> as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: mirage\n  monitor: hvt\n  architecture: x86\n\nkernel:\n  from: local\n  path: network.hvt\n\ncmdline: \"\"\n</code></pre> <p>In the above file we specify the following:</p> <ul> <li>We want to package a MirageOS unikernel that   will execute on top of Solo5-hvt over x86   architecture.</li> <li>We want to use the <code>network.hvt</code> binary as the unikernel to boot.</li> <li>We do not specify any command line, since the unikernel does not necessarily require one.</li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f bunnyfile -t urunc/prebuilt/network-mirage-hvt:test .\n</code></pre>"},{"location":"package/pre-built/#using-a-dockerfile-like-syntax","title":"Using a Dockerfile-like syntax","text":"<p>In order to package an existing pre-built unikernel image with bunny and a Dockerfile-like syntax file, we can define the <code>Containerfile</code> as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nFROM scratch\n\nCOPY network.hvt /unikernel/network.hvt\n\nLABEL com.urunc.unikernel.binary=/unikernel/network.hvt\nLABEL \"com.urunc.unikernel.cmdline\"=\"\"\nLABEL \"com.urunc.unikernel.unikernelType\"=\"mirage\"\nLABEL \"com.urunc.unikernel.hypervisor\"=\"hvt\"\nLABEL \"com.urunc.unikernel.useDMBlock\"=\"false\"\n</code></pre> <p>In the above file:</p> <ul> <li>We directly copy the unikernel binary in the OCI's image rootfs.</li> <li>We manually specify through labels the <code>urunc</code> annotations.</li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f Containerfile -t urunc/prebuilt/network-mirage-hvt:test .\n</code></pre>"},{"location":"package/pre-built/#using-bunix","title":"Using <code>bunix</code>","text":"<p>In the case of bunix we need to clone the whole repository in the same directly as the unikernel. Then, we simply need to edit the <code>args.nix</code> file as:</p> <pre><code>{\n  name = \"urunc/prebuilt/network-mirage-hvt\";\n  tag = \"test\";\n  files = {\n    \"./network.hvt\" = \"/unikernel/network.hvt\";\n  };\n  annotations = {\n    unikernelType = \"mirage\";\n    hypervisor = \"hvt\";\n    binary = \"/unikernel/network.hvt\";\n    cmdline = \"\";\n    unikernelVersion = \"\";\n    initrd = \"\";\n    block = \"\";\n    blkMntPoint = \"\";\n    useDMBlock = \"false\";\n  };\n}\n</code></pre> <p>In the above file:</p> <ul> <li>We directly specify the files to copy inside the OCI's image rootfs.</li> <li>We manually specify all <code>urunc</code> annotations.</li> </ul> <p>We can build the OCI image by simply running the following command:</p> <pre><code>nix-build default.nix\n</code></pre> <p>The above command will create a container image in a tar inside Nix's store. For easier access of the tar, Nix creates a symlink of the tar file in the CWD. The symlink will be named as <code>result</code>. Therefore, we can load the container image with:</p> <pre><code>docker load &lt; result\n</code></pre>"},{"location":"package/reuse/","title":"Reusing OCI images that contain unikernels","text":"<p>In this page we will explain how we can reuse existing OCI images that contain unikernels to either update or append <code>urunc</code> annotations. As an example, we will use an existing Unikraft Unikernel image from Unikraft's catalog, The goal will be to transform this image to an OCI image that <code>urunc</code> can handle, by simply appending the necessary annotations.</p> <p>Currently only <code>bunny</code> supports reusing an existing OCI image. However, both file formats, <code>bunnyfile</code> and Dockerfile-like syntax files, can be used.</p> <p>NOTE: The below steps can be easily adjusted to any existing OCI image.</p>"},{"location":"package/reuse/#using-a-bunnyfile","title":"Using a <code>bunnyfile</code>","text":"<p>In order to append <code>urunc</code> annotations in an existing Unikraft OCI image, we can define the <code>bunnyfile</code> as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: unikraft\n  monitor: qemu\n  architecture: x86\n\nkernel:\n  from: unikraft.org/nginx:1.15\n  path: /unikraft/bin/kernel\n\ncmdline: \"nginx -c /nginx/conf/nginx.conf\"\n</code></pre> <p>In the above file we specify the followings:</p> <ul> <li>We want to use a Unikraft unikernel that will execute on top of Qemu over x86   architecture.</li> <li>We want to use the unikernel binary <code>/unikraft/bin/kernel</code> from the   <code>unikraft.org/nginx:1.15</code> OCI image.</li> <li>We specify the cmdline for the unikernel as <code>nginx -c /nginx/conf/nginx.conf\"</code></li> </ul> <p>With the above file, <code>bunny</code> will fetch the OCI image and append the <code>urunc</code> annotations. We can build the OCI image with the following command:</p> <pre><code>docker build -f bunnyfile -t urunc/reuse/nginx-unikraft-qemu:test .\n</code></pre>"},{"location":"package/reuse/#using-a-dockerfile-like-syntax","title":"Using a Dockerfile-like syntax","text":"<p>In the case of the Dockerfile-like syntax file, we need to manually specify the <code>urunc</code> annotations, using the respective labels. Therefore, to transform the above <code>bunnyfile</code> to the equivalent <code>Containerfile</code>:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nFROM unikraft.org/nginx:1.15\n\nLABEL com.urunc.unikernel.binary=\"/unikraft/bin/kernel\"\nLABEL \"com.urunc.unikernel.cmdline\"=\"nginx -c /nginx/conf/nginx.conf\"\nLABEL \"com.urunc.unikernel.unikernelType\"=\"unikraft\"\nLABEL \"com.urunc.unikernel.hypervisor\"=\"qemu\"\n</code></pre> <p>In the above file:</p> <ul> <li>We set the <code>unikraft.org/nginx:1.15</code> as the base for our OCI image.</li> <li>We manually specify through labels the <code>urunc</code> annotations.</li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f Containerfile -t urunc/prebuilt/nginx-unikraft-qemu:test .\n</code></pre>"},{"location":"package/rootfs/","title":"Packaging and creating unikernel's rootfs for <code>urunc</code>","text":"<p>The unikernel and libOS ecosystem is highly diverse, with each framework offering its own approach to storage. The users can easily get lost on the various storage technologies that each framework supports.  This challenge was one of the key reasons we created bunny and bunix, in an effort to simplify the process and provide a unified interface for managing storage across all unikernel frameworks. On this page, we will explore the current state of our tools and explain how to use them to create and package a root filesystem (rootfs) for a unikernel.</p> <p>For the time being, <code>urunc</code> supports two ways for passing the rootfs to the unikernel: a) through initrd and b) as a virtio-block. In the latter case, <code>urunc</code> can either levarage the container's snapshot and pass the whole container's rootfs as the rootfs, or <code>urunc</code> can make use of a user-created file inside the OCI image to pass as a virtio-block device to the unikernel.</p> <p>Therefore, the users have the following options:</p> <ol> <li>Manually create a rootfs (either initrd or block) and package it along with    the unikernel.</li> <li>Directly copy all the files to the container's rootfs and use devmapper    snapshotter, in order to allow <code>urunc</code> to pass the snapshot as a virtio-block    to the unikernel.</li> <li>Let bunny and bunix create the rootfs file.</li> </ol> <p>NOTE: For the time being, bunny supports the creation of initrd files and bunix does not provide any support for creating the rootfs.</p>"},{"location":"package/rootfs/#creating-an-initrd-file","title":"Creating an initrd file","text":"<p>Some unikernel frameworks and guests support an in-memory ramfs as a rootfs. In these cases, we can use bunny and instruct it to create the rootfs for us with all the specified files. This feature is only supported using a <code>bunnyfile</code> and not a Dockerfile-like syntax file.</p> <p>Let's take a look at it using a Redis Unikraft unikernel as an example, targeting Qemu. We will define the <code>bunnyfile</code> as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: unikraft\n  monitor: qemu\n  architecture: x86\n\nrootfs:\n  from: scratch\n  type: initrd\n  include:\n  - redis.conf:/conf/redis.conf\n\nkernel:\n  from: local\n  path: redis-qemu-x86_64-initrd_qemu-x86_64\n\ncmdline: \"redis-server /conf/redis.conf\"\n</code></pre> <p>In the above file we specify the following:</p> <ul> <li>We want to package a Unikraft unikernel that will execute on top of Qemu over   x86 architecture.</li> <li>We want to create from <code>scratch</code> a rootfs with <code>initrd</code> as its type. In   particular, we want a initrd file that contains the file <code>redis.conf</code> in   <code>/data/conf/redis.conf</code>. In that way, bunny creates the initrd file for us   and sets up the respective <code>urunc</code> annotations to attach this initrd file   when we boot the unikernel.</li> <li>We want to use the <code>redis-qemu-x86_64-initrd_qemu-x86_64</code> binary from the local build context as the unikernel to boot.</li> <li>We specify the cmdline for the unikernel as <code>redis-server /conf/redis.conf</code></li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f bunnyfile -t urunc/prebuilt/redis-unikraft-qemu:test .\n</code></pre>"},{"location":"package/rootfs/#preparing-an-oci-image-to-be-used-as-a-rootfs-for-the-unikernel","title":"Preparing an OCI image to be used as a rootfs for the unikernel","text":"<p>As previously mentioned, <code>urunc</code> is able to pass the whole container's rootfs as the rootfs for the unikernels that support virtio-block. In that scenario, we simply need to copy any local files to the OCI image's rootfs. For this scenario we can use both bunny and bunix. It is important to note that we need to create the unikernel container using devmapper as a snapshotter. In that way,<code>urunc</code> will use the snapshot of the container and directly attach it to the unikernel as  a virtio-block device.</p> <p>As an example, we will use a Redis Rumprun unikernel from Rumprun-packages targeting Solo5-hvt.</p> <p>NOTE: Rumprun does not support attaching a virtio-block directly to <code>/</code>, hence <code>urunc</code> will instruct Rumprun to mount it at <code>/data</code>.</p>"},{"location":"package/rootfs/#using-bunny","title":"Using <code>bunny</code>","text":"<p>In the case of bunny, we can use both supported file syntaxes: a) <code>bunnyfile</code> and b) the Dockerfile-like syntax.</p>"},{"location":"package/rootfs/#using-a-bunnyfile","title":"Using a <code>bunnyfile</code>","text":"<p>In order to package an existing pre-built unikernel image and any other files with bunny and a <code>bunnyfile</code> we can define the <code>bunnyfile</code> as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: rumprun\n  monitor: hvt\n  architecture: x86\n\nrootfs:\n  from: scratch\n  type: raw\n  include:\n  - redis.conf:/conf/redis.conf\n\nkernel:\n  from: local\n  path: redis.hvt\n\ncmdline: \"redis-server /data/conf/redis.conf\"\n</code></pre> <p>In the above file we specify the following:</p> <ul> <li>We want to package a Rumprun unikernel   that will execute on top of Solo5-hvt over x86   architecture.</li> <li>We want to create a rootfs from <code>scratch</code> with a <code>raw</code> type, meaning that   we will just copy the   specified files directly to the OCI image's rootfs. In particular, we copy the   file <code>redis.conf</code> and place it at <code>/conf/redis.conf</code>.This is similar to   <code>COPY</code> in Dockerfile.  Because of this type selection, bunny will also set up   the respective annotations to mount the OCI images rootfs directly to the   unikernel.</li> <li>We want to use the <code>redis.hvt</code> binary as the unikernel to boot.</li> <li>We specify the cmdline for the unikernel as <code>redis-server  /data/conf/redis.conf</code></li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f bunnyfile -t urunc/prebuilt/redis-rumprun-hvt:test .\n</code></pre>"},{"location":"package/rootfs/#using-a-dockerfile-like-syntax","title":"Using a Dockerfile-like syntax","text":"<p>In order to package an existing pre-built unikernel image and any other files with bunny using a Dockerfile-like syntax file, we can define the <code>Containerfile</code> as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:0.2.0\nFROM scratch\n\nCOPY redis.hvt /unikernel/redis.hvt\nCOPY redis.conf /conf/redis.conf\n\nLABEL com.urunc.unikernel.binary=/unikernel/redis.hvt\nLABEL \"com.urunc.unikernel.cmdline\"=\"redis-server /data/conf/redis.conf\"\nLABEL \"com.urunc.unikernel.unikernelType\"=\"rumprun\"\nLABEL \"com.urunc.unikernel.hypervisor\"=\"hvt\"\nLABEL \"com.urunc.unikernel.useDMBlock\"=\"true\"\n</code></pre> <p>In the above file:</p> <ul> <li>We directly copy the unikernel binary and any files that we want to have in   the OCI's image rootfs.</li> <li>We manually specify through labels the necessary <code>urunc</code> annotations,   including the <code>com.urunc.unikernel.useDMBlock</code> which instructs <code>urunc</code> to   attach the container snapshot as a virtio-block device for the unikernel.</li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f Containerfile -t urunc/prebuilt/redis-rumprun-hvt:test .\n</code></pre>"},{"location":"package/rootfs/#using-bunix","title":"Using <code>bunix</code>","text":"<p>In the case of bunix we need the whole repository in the same directly as the unikernel. Then, we simply need to edit the <code>args.nix</code> file. For our pre-built Redis Rumprun unikernel we can define the files as:</p> <pre><code>{\n  name = \"urunc/prebuilt/redis-rumprun-hvt\";\n  tag = \"test\";\n  files = {\n    \"./redis.hvt\" = \"/unikernel/redis.hvt\";\n    \"./redis.conf\" = \"/conf/redis.conf\";\n  };\n  annotations = {\n    unikernelType = \"rumprun\";\n    hypervisor = \"hvt\";\n    binary = \"/unikernel/redis.hvt\";\n    cmdline = \"hello\";\n    unikernelVersion = \"\";\n    initrd = \"\";\n    block = \"\";\n    blkMntPoint = \"\";\n    useDMBlock = \"true\";\n  };\n}\n</code></pre> <p>In the above file:</p> <ul> <li>We directly specify the files to copy inside the OCI's image rootfs.</li> <li>We manually specify through labels the necessary <code>urunc</code> annotations,   including the <code>com.urunc.unikernel.useDMBlock</code> which instructs <code>urunc</code> to   attach the container snapshot as a virtio-block device for the unikernel.</li> </ul> <p>We can build the OCI image by simply running the following command:</p> <pre><code>nix-build default.nix\n</code></pre> <p>The above command will create a container image in a tar inside Nix's store. For easier access of the tar, Nix creates a symlink of the tar file in the CWD. The symlink will be named as <code>result</code>. Therefore, we can load the container image with:</p> <pre><code>docker load &lt; result\n</code></pre>"},{"location":"package/rootfs/#packaging-a-pre-built-rootfs-along-with-the-unikernel","title":"Packaging a pre-built rootfs along with the unikernel","text":"<p>At last, there is always the option to manually create the rootfs file for the unikernel and then package the unikernel binary and the rootfs file setting up the respective annotations.</p> <p>As an example, we will use a simple  C HTTP Web Server from Unikraft's catalog.</p>"},{"location":"package/rootfs/#using-bunny_1","title":"Using <code>bunny</code>","text":"<p>In the case of bunny, we can use both supported file syntaxes: a) <code>bunnyfile</code> and b) the Dockerfile-like syntax.</p>"},{"location":"package/rootfs/#using-a-bunnyfile_1","title":"Using a <code>bunnyfile</code>","text":"<p>In order to package an existing pre-built unikernel and its rootfs with bunny and a <code>bunnyfile</code> we can define the <code>bunnyfile</code> as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: unikraft\n  monitor: qemu\n  architecture: x86\n\nrootfs:\n  from: local\n  path: rootfs.cpio\n\nkernel:\n  from: local\n  path: app-elfloader-qemu-x86_64-initrd_qemu-x86_64\n\ncmdline: \"/chttp\"\n</code></pre> <p>In the above file we specify the following:</p> <ul> <li>We want to package a Unikraft unikernel that will execute   on top of Qemu over x86 architecture.</li> <li>We want to use a local file as a rootfs, specifically the <code>rootfs.cpio</code> file   in the local build context.</li> <li>We want to use the <code>app-elfloader-qemu-x86_64-initrd_qemu-x86_64</code> binary as   the unikernel to boot.</li> <li>We specify the cmdline for the unikernel as <code>/chttp</code></li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f bunnyfile -t urunc/prebuilt/chttp-unikraft-qemu:test .\n</code></pre>"},{"location":"package/rootfs/#using-a-dockerfile-like-syntax_1","title":"Using a Dockerfile-like syntax","text":"<p>We can do all the above using a Dockerfile-like syntax file as:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nFROM scratch\n\nCOPY app-elfloader-qemu-x86_64-initrd_qemu-x86_64 /unikernel/kernel\nCOPY rootfs.cpio /unikernel/rootfs.cpio\n\nLABEL \"com.urunc.unikernel.binary\"=/unikernel/kernel\nLABEL \"com.urunc.unikernel.initrd\"=\"/unikernel/rootfs.cpio\"\nLABEL \"com.urunc.unikernel.cmdline\"=\"/chttp\"\nLABEL \"com.urunc.unikernel.unikernelType\"=\"unikraft\"\nLABEL \"com.urunc.unikernel.hypervisor\"=\"qemu\"\n</code></pre> <p>In the above file:</p> <ul> <li>We directly copy the unikernel binary and the cpio file in   the OCI's image rootfs.</li> <li>We manually specify all <code>urunc</code> annotations, including the initrd one to   specify the file to use as initrd for the unikernel.</li> </ul> <p>We can build the OCI image with the following command:</p> <pre><code>docker build -f Containerfile -t urunc/prebuilt/chttp-unikraft-qemu:test .\n</code></pre>"},{"location":"package/rootfs/#using-bunix_1","title":"Using <code>bunix</code>","text":"<p>In the case of bunix we need the whole repository in the same directory as the unikernel and the cpio file. Then, we simply need to edit the <code>args.nix</code> file as:</p> <pre><code>{\n  name = \"urunc/prebuilt/chttp-unikraft-qemu\";\n  tag = \"test\";\n  files = {\n    \"./app-elfloader-qemu-x86_64-initrd_qemu-x86_64\" = \"/unikernel/kernel\";\n    \"./rootfs.cpio\" = \"/unikernel/rootfs.cpio\";\n  };\n  annotations = {\n    unikernelType = \"unikraft\";\n    hypervisor = \"qemu\";\n    binary = \"/unikernel/kernel\";\n    cmdline = \"/chttp\";\n    unikernelVersion = \"\";\n    initrd = \"/unikernel/rootfs.cpio\";\n    block = \"\";\n    blkMntPoint = \"\";\n    useDMBlock = \"\";\n  };\n}\n</code></pre> <p>In the above file:</p> <ul> <li>We directly specify the files to copy inside the OCI's image rootfs.</li> <li>We manually specify all <code>urunc</code> annotations, including the initrd one to   specify the file to use as initrd for the unikernel.</li> </ul> <p>We can build the OCI image by simply running the following command:</p> <pre><code>nix-build default.nix\n</code></pre> <p>The above command will create a container image in a tar inside Nix's store. For easier access of the tar, Nix creates a symlink of the tar file in the CWD. The symlink will be named as <code>result</code>. Therefore, we can load the container image with:</p> <pre><code>docker load &lt; result\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>In this section we include some end-to-end tutorials on deploying <code>urunc</code>-compatible workloads to various environments:</p> <ul> <li><code>urunc</code> in Kubernetes</li> <li><code>urunc</code> in EKS</li> <li>Knative integration</li> <li>Non root monitor execution</li> <li>Running existing containers over Linux</li> </ul>"},{"location":"tutorials/How-to-urunc-on-k8s/","title":"How to use urunc with k8s","text":"<p>This guide assumes you have a working Kubernetes cluster.</p> <p>To use <code>urunc</code> in a k8s cluster there are 2 options:</p> <ul> <li>Manual installation</li> <li>Using urunc-deploy</li> </ul>"},{"location":"tutorials/How-to-urunc-on-k8s/#manual-installation","title":"Manual Installation","text":""},{"location":"tutorials/How-to-urunc-on-k8s/#install-urunc","title":"Install urunc","text":"<p>Before we start, we need to have working Kubernetes cluster with urunc installed on one or more nodes.</p>"},{"location":"tutorials/How-to-urunc-on-k8s/#add-urunc-as-a-runtimeclass","title":"Add urunc as a RuntimeClass","text":"<p>First, we need to add <code>urunc</code> as a runtime class for the k8s cluster:</p> <pre><code>cat &lt;&lt; EOF | tee urunc-runtimeClass.yaml\nkind: RuntimeClass\napiVersion: node.k8s.io/v1\nmetadata:\n    name: urunc\nhandler: urunc\nEOF\n\nkubectl apply -f urunc-runtimeClass.yaml\n</code></pre> <p>To verify the runtimeClass was added:</p> <pre><code>kubectl get runtimeClass\n</code></pre>"},{"location":"tutorials/How-to-urunc-on-k8s/#create-a-test-deployment","title":"Create a test deployment","text":"<p>To properly test the newly added k8s runtime class, create a test deployment:</p> <pre><code>cat &lt;&lt;EOF | tee nginx-urunc.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: nginx-urunc\n  name: nginx-urunc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: nginx-urunc\n  template:\n    metadata:\n      labels:\n        run: nginx-urunc\n    spec:\n      runtimeClassName: urunc\n      containers:\n      - image: harbor.nbfc.io/nubificus/urunc/nginx-firecracker-unikraft-initrd:latest\n        imagePullPolicy: Always\n        name: nginx-urunc\n        command: [\"sleep\"]\n        args: [\"infinity\"]\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 10m\n      restartPolicy: Always\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-urunc\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    run: nginx-urunc\n  sessionAffinity: None\n  type: ClusterIP\nEOF\n\nkubectl apply -f nginx-urunc.yaml\n</code></pre> <p>Now, we should be able to see the created Pod:</p> <pre><code>kubectl get pods\n</code></pre>"},{"location":"tutorials/How-to-urunc-on-k8s/#urunc-deploy","title":"urunc-deploy","text":"<p><code>urunc-deploy</code> provides a Dockerfile, which contains all of the binaries and artifacts required to run <code>urunc</code>, as well as reference DaemonSets, which can be utilized to install <code>urunc</code> runtime  on a running Kubernetes cluster.</p>"},{"location":"tutorials/How-to-urunc-on-k8s/#urunc-deploy-in-k3s","title":"urunc-deploy in k3s","text":"<p>To install in a k3s cluster, first we need to create the RBAC:</p> <pre><code>git clone https://github.com/urunc-dev/urunc.git\ncd urunc\nkubectl apply -f deployment/urunc-deploy/urunc-rbac/urunc-rbac.yaml\n</code></pre> <p>Then, we create the <code>urunc-deploy</code> Daemonset, followed by the k3s customization:</p> <pre><code>kubectl apply -k deployment/urunc-deploy/urunc-deploy/overlays/k3s\n</code></pre> <p>Finally, we need to create the appropriate k8s runtime class:</p> <pre><code>kubectl apply -f deployment/urunc-deploy/runtimeclasses/runtimeclass.yaml\n</code></pre> <p>To uninstall:</p> <pre><code>kubectl delete -k deployment/urunc-deploy/urunc-deploy/overlays/k3s\nkubectl apply -k deployment/urunc-deploy/urunc-cleanup/overlays/k3s\n</code></pre> <p>After the cleanup is completed and the <code>urunc-deploy</code> Pod is terminated:</p> <pre><code>kubectl delete -k deployment/urunc-deploy/urunc-cleanup/overlays/k3s\nkubectl delete -f deployment/urunc-deploy/urunc-rbac/urunc-rbac.yaml\nkubectl delete -f deployment/urunc-deploy/runtimeclasses/runtimeclass.yaml\n</code></pre>"},{"location":"tutorials/How-to-urunc-on-k8s/#urunc-deploy-in-k8s-with-containerd","title":"urunc-deploy in k8s with containerd","text":"<p>To install in a k8s cluster, first we need to create the RBAC:</p> <pre><code>git clone https://github.com/urunc-dev/urunc.git\ncd urunc\nkubectl apply -f deployment/urunc-deploy/urunc-rbac/urunc-rbac.yaml\n</code></pre> <p>Then, we create the <code>urunc-deploy</code> Daemonset:</p> <pre><code>kubectl apply -f deployment/urunc-deploy/urunc-deploy/base/urunc-deploy.yaml\n</code></pre> <p>Finally, we need to create the appropriate k8s runtime class:</p> <pre><code>kubectl apply -f deployment/urunc-deploy/runtimeclasses/runtimeclass.yaml\n</code></pre> <p>To uninstall:</p> <pre><code>kubectl delete -f deployment/urunc-deploy/urunc-deploy/base/urunc-deploy.yaml\nkubectl apply -f deployment/urunc-deploy/urunc-cleanup/base/urunc-cleanup.yaml\n</code></pre> <p>After the cleanup is completed:</p> <pre><code>kubectl delete -f deployment/urunc-deploy/urunc-cleanup/base/urunc-cleanup.yaml\nkubectl delete -f deployment/urunc-deploy/urunc-rbac/urunc-rbac.yaml\nkubectl delete -f deployment/urunc-deploy/runtimeclasses/runtimeclass.yaml\n</code></pre> <p>Now, we can create new <code>urunc</code> deployments using the instruction provided in manual installation.</p>"},{"location":"tutorials/How-to-urunc-on-k8s/#how-urunc-deploy-works","title":"How urunc-deploy works","text":"<p><code>urunc-deploy</code> consists of several components and steps that install <code>urunc</code> along with the supported hypervisors, configure <code>containerd</code> and Kubernetes (k8s) to use <code>urunc</code>, and provide a simple way to remove those components from the cluster.</p> <p>During installation, the following steps take place:</p> <ul> <li>A RBAC role is created to allow <code>urunc-deploy</code> to run with privileged access.</li> <li>The <code>urunc-deploy</code> Pod is deployed with privileges on the host, and the <code>containerd</code> configuration is mounted inside the Pod.</li> <li><code>urunc-deploy</code> performs the following tasks:<ul> <li>Copies <code>urunc</code> and hypervisor binaries to the host under <code>usr/local/bin</code>.</li> <li>Creates a backup of the current <code>containerd</code> configuration file.</li> <li>Edits the <code>containerd</code> configuration file to add <code>urunc</code> as a supported runtime.</li> <li>Restarts <code>containerd</code>, if necessary.</li> <li>Labels the Node with label <code>urunc.io/urunc-runtime=true</code>.</li> </ul> </li> <li>Finally, <code>urunc</code> is added as a runtime class in k8s.</li> </ul> <p>Note: <code>urunc-deploy</code> will install a static version of QEMU in <code>/usr/local/bin/</code> along with the QEMU BIOS files in <code>/usr/local/share/</code>. Therefore, files with the same names under these directories will get overwritten.</p> <p>During cleanup, these changes are reverted:</p> <ul> <li>The <code>urunc</code> and hypervisor binaries are deleted.</li> <li>The <code>containerd</code> configuration file is restored to the pre-<code>urunc-deploy</code> state.</li> <li>The <code>urunc.io/urunc-runtime=true</code> label is removed from the Node.</li> <li>The RBAC role, the <code>urunc-deploy</code> Pod and the runtime class are removed.</li> </ul>"},{"location":"tutorials/Non-root-monitor-execution/","title":"Non-root execution of monitor","text":"<p>To enhance security, <code>urunc</code> supports running the monitor process (VMM or seccomp monitor) as a non-root user. This can be as simple as setting the respective uid/gid for the execution of the container.</p>"},{"location":"tutorials/Non-root-monitor-execution/#running-the-monitor-as-non-root-user","title":"Running the monitor as non-root user","text":"<p>By default <code>urunc</code> will execute the monitor setting up the <code>uid</code>, <code>gid</code> and <code>additionalGids</code> from the container's OCI configuration. As a result, we simply need to instruct <code>urunc</code> to run a container as the desired user.</p>"},{"location":"tutorials/Non-root-monitor-execution/#docker-and-nerdctl","title":"Docker and Nerdctl","text":"<p>In the case of docker and nerdctl, we can set the user and the groups of the container with the <code>--user &lt;uid&gt;:&lt;gid&gt;</code> option and the additional groups using <code>--group-add &lt;gid&gt;</code> for each additional group. Therefore, to run a KVM-enabled monitor with <code>urunc</code> as <code>nobody</code>, we use the following command:</p> <pre><code>sudo nerdctl run  --user 65534:65534 --runtime \"io.containerd.urunc.v2\" --rm -it harbor.nbfc.io/nubificus/urunc/nginx-firecracker-unikraft-initrd:latest\n</code></pre> <p>Note The commands are the same for docker.</p>"},{"location":"tutorials/Non-root-monitor-execution/#in-a-k8s-deployment","title":"In a k8s deployment","text":"<p>Similarly, in the case of Kubernetes, we can specify the monitor's process user and groups by defining the container's user and groups. We can do that in the <code>securityContext</code> field of the deployment yaml:</p> <pre><code>securityContext:\n  runAsUser: 65534\n  runAsGroup: 65534\n  supplementalGroups: [1000]\n</code></pre> <p>For more information regarding the Security Context of a Pod / Container take a look at Kubernetes's documentation.</p>"},{"location":"tutorials/eks-tutorial/","title":"EKS Setup for <code>urunc</code>","text":"<p>In this tutorial, we\u2019ll walk through the complete process of provisioning an Amazon EKS (Elastic Kubernetes Service) cluster from scratch using the AWS CLI, <code>eksctl</code>, and a few supporting tools.</p> <p>Our goal is to create a Kubernetes-native environment capable of securely running containers with <code>urunc</code> \u2014 a unikernel container runtime. This tutorial sets up a production-grade EKS cluster, complete with custom networking, Calico CNI plugin for fine-grained pod networking, and node groups ready to schedule unikernel workloads.</p> <p>We\u2019ll cover:</p> <ul> <li>Tooling prerequisites</li> <li>VPC and networking setup</li> <li>Cluster bootstrapping with <code>eksctl</code></li> <li>Calico installation and configuration</li> <li>Managed node group provisioning</li> <li><code>urunc</code> installation</li> <li>Example deployment of unikernels</li> </ul>"},{"location":"tutorials/eks-tutorial/#tooling-setup-for-eks-cluster-provisioning","title":"Tooling Setup for EKS Cluster Provisioning","text":"<p>This section ensures your local environment is equipped with all the required tools to interact with AWS and provision your EKS cluster.</p>"},{"location":"tutorials/eks-tutorial/#prerequisites","title":"Prerequisites","text":"<p>You'll need the following CLI tools installed and configured:</p>"},{"location":"tutorials/eks-tutorial/#1-aws-cli","title":"1. AWS CLI","text":"<p>Used to interact with AWS services like IAM, EC2, CloudFormation, etc.</p> <p>Install AWS CLI (v2 recommended) <pre><code>curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\nunzip awscliv2.zip\nsudo ./aws/install\n</code></pre></p> <p>Verify installation: <pre><code>aws --version\n</code></pre></p> <p>Configure it with your credentials: <pre><code>aws configure\n</code></pre></p> <p>You'll be prompted to enter:</p> <ul> <li>AWS Access Key ID </li> <li>AWS Secret Access Key</li> <li>Default region (e.g., <code>eu-central-1</code>)</li> <li>Default output format (e.g., <code>json</code>)</li> </ul>"},{"location":"tutorials/eks-tutorial/#2-eksctl","title":"2. eksctl","text":"<p>The official CLI tool for managing EKS clusters.</p> <p>Download and install eksctl:</p> <pre><code>curl --silent --location \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" | tar xz -C /tmp\nsudo mv /tmp/eksctl /usr/local/bin\n</code></pre> <p>Verify the installation: <pre><code>eksctl version\n</code></pre></p>"},{"location":"tutorials/eks-tutorial/#3-kubectl","title":"3. kubectl","text":"<p>The Kubernetes CLI used to interact with your EKS cluster.</p> <p>Install kubectl (replace version as needed):</p> <pre><code>curl -LO \"https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl\"\nchmod +x kubectl\nsudo mv kubectl /usr/local/bin/\n</code></pre> <p>Verify the installation: <pre><code>kubectl version --client\n</code></pre></p>"},{"location":"tutorials/eks-tutorial/#4-jq","title":"4. jq","text":"<p>A lightweight and flexible command-line JSON processor, used in helper scripts.</p> <pre><code>sudo apt-get update\nsudo apt-get install -y jq\n</code></pre>"},{"location":"tutorials/eks-tutorial/#5-ssh-keypair-for-node-access","title":"5. SSH Keypair (for Node Access)","text":"<p>Ensure you have a key pair uploaded to AWS for SSH access to EC2 instances.</p> <p>Generate an SSH key if you don\u2019t have one: <pre><code>ssh-keygen -t rsa -b 4096 -f ~/.ssh/awseks -N \"\"\n</code></pre> Import the public key into AWS (or use an existing one)</p> <pre><code>aws ec2 import-key-pair \\\n  --key-name awseks \\\n  --public-key-material fileb://~/.ssh/awseks.pub\n</code></pre>"},{"location":"tutorials/eks-tutorial/#cluster-setup","title":"Cluster Setup","text":"<p>We begin by provisioning an Amazon EKS cluster with private subnets and Calico as the CNI instead of the default AWS CNI.</p>"},{"location":"tutorials/eks-tutorial/#vpc-with-private-subnets","title":"VPC with Private Subnets","text":"<p>We use the official EKS CloudFormation template to create a VPC with private subnets.</p> <pre><code>export STACK_NAME=\"urunc-tutorial\"\nexport REGION=\"eu-central-1\"\naws cloudformation create-stack \\\n  --region $REGION \\\n  --stack-name $STACK_NAME \\\n  --template-url https://s3.us-west-2.amazonaws.com/amazon-eks/cloudformation/2020-10-29/amazon-eks-vpc-private-subnets.yaml\n</code></pre> <p>The output of the above command would verify the successful creation of the VPC:</p> <pre><code>{\n    \"StackId\": \"arn:aws:cloudformation:eu-central-1:058264306458:stack/urunc-tutorial/ec8ae800-0fbc-11f0-bda2-0a29df3fde61\"\n}\n</code></pre>"},{"location":"tutorials/eks-tutorial/#create-iam-role-for-the-eks-cluster","title":"Create IAM Role for the EKS Cluster","text":"<p>We define a trust policy allowing EKS to assume a role. Create a json file (e.g. <code>eks-cluster-role-trust-policy.json</code>) with the following contents: <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"eks.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n</code></pre></p> <p>Create the role:</p> <pre><code>aws iam create-role \\\n  --role-name uruncTutorialRole \\\n  --assume-role-policy-document file://eks-cluster-role-trust-policy.json\n</code></pre> <p>Attach the required EKS policy: <pre><code>aws iam attach-role-policy \\\n  --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy \\\n  --role-name uruncTutorialRole\n</code></pre></p>"},{"location":"tutorials/eks-tutorial/#extract-public-subnet-ids-if-needed","title":"Extract Public Subnet IDs (if needed)","text":"<p>This helper script (<code>get_pub_subnets.sh</code>) identifies public subnets in the current region by checking for routes to an Internet Gateway:</p> <pre><code>#!/bin/bash\n\nREGION=\"eu-central-1\"\nsubnets=$(aws ec2 describe-subnets --query 'Subnets[*].{ID:SubnetId}' --output text --region $REGION)\nroute_tables=$(aws ec2 describe-route-tables --query 'RouteTables[*].{ID:RouteTableId,Associations:Associations[*].SubnetId,Routes:Routes[*]}' --output json --region $REGION)\n\npublic_subnets=()\n\nfor subnet in $subnets; do\n  associated_route_table=$(echo $route_tables | jq -r --arg SUBNET \"$subnet\" '.[] | select(.Associations[]? == $SUBNET) | .ID')\n  if [ -n \"$associated_route_table\" ]; then\n    has_igw=$(echo $route_tables | jq -r --arg RTID \"$associated_route_table\" '.[] | select(.ID == $RTID) | .Routes[] | select(.GatewayId != null) | .GatewayId' | grep 'igw-')\n    if [ -n \"$has_igw\" ]; then\n      public_subnets+=(\"$subnet\")\n    fi\n  fi\ndone\n\npublic_subnets_str=$(IFS=,; echo \"${public_subnets[*]}\")\necho \"$public_subnets_str\"\n</code></pre> <p>Run it to retrieve subnet IDs: <pre><code>bash get_pub_subnets.sh\n</code></pre></p> <p>Example output: <pre><code>subnet-02bcaca5ac39eca7a,subnet-0d0667e2156169998\n</code></pre></p>"},{"location":"tutorials/eks-tutorial/#create-the-eks-cluster-with-calico-cni","title":"Create the EKS Cluster with Calico CNI","text":"<p>It is time to set up the cluster and managed node groups with Calico networking.</p>"},{"location":"tutorials/eks-tutorial/#step-1-create-eks-control-plane-with-private-subnets-and-no-initial-node-group","title":"Step 1: Create EKS control plane with private subnets and no initial node group","text":"<p>Use the subnets from the command above.</p> <pre><code>export CLUSTER_NAME=\"urunc-tutorial\"\nexport REGION=\"eu-central-1\"\nexport SUBNETS=\"subnet-02bcaca5ac39eca7a,subnet-0d0667e2156169998\"\neksctl create cluster \\\n  --name ${CLUSTER_NAME} \\\n  --region $REGION \\\n  --version 1.30 \\\n  --vpc-private-subnets $SUBNETS \\\n  --without-nodegroup\n</code></pre> <p>Example output: <pre><code>2 sequential tasks: { create cluster control plane \"urunc-tutorial\", wait for control plane to become ready\n}\n2025-04-02 12:29:16 [\u2139]  building cluster stack \"eksctl-urunc-tutorial-cluster\"\n2025-04-02 12:29:19 [\u2139]  deploying stack \"eksctl-urunc-tutorial-cluster\"\n2025-04-02 12:29:49 [\u2139]  waiting for CloudFormation stack \"eksctl-urunc-tutorial-cluster\"\n[...]\n2025-04-02 12:39:26 [\u2139]  waiting for the control plane to become ready\n2025-04-02 12:39:27 [\u2714]  saved kubeconfig as \"~/.kube/config\"\n2025-04-02 12:39:27 [\u2139]  no tasks\n2025-04-02 12:39:27 [\u2714]  all EKS cluster resources for \"urunc-tutorial\" have been created\n2025-04-02 12:39:27 [\u2714]  created 0 nodegroup(s) in cluster \"urunc-tutorial\"\n2025-04-02 12:39:27 [\u2714]  created 0 managed nodegroup(s) in cluster \"urunc-tutorial\"\n2025-04-02 12:39:35 [\u2139]  kubectl command should work with \"~/.kube/config\", try 'kubectl get nodes'\n2025-04-02 12:39:35 [\u2714]  EKS cluster \"urunc-tutorial\" in \"eu-central-1\" region is ready\n</code></pre></p> <p>Now, you should have the control-plane deployed and ready. The first thing to do is to remove the AWS CNI, as gateway ARP entries are statically populated.</p>"},{"location":"tutorials/eks-tutorial/#step-2-remove-aws-cni","title":"Step 2: Remove AWS CNI","text":"<pre><code>kubectl delete daemonset -n kube-system aws-node\n</code></pre> <p>Expected output: <pre><code>daemonset.apps \"aws-node\" deleted\n</code></pre></p>"},{"location":"tutorials/eks-tutorial/#step-3-add-calico-cni","title":"Step 3: Add Calico CNI:","text":"<pre><code>kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/tigera-operator.yaml\n</code></pre> <p>Note: There are cases where a large set of manifests can cause a failure to the above command. If it does, try to re-issue the command.</p> <p>Expected output: <pre><code>namespace/tigera-operator created\ncustomresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created\ncustomresourcedefinition.apiextensions.k8s.io/apiservers.operator.tigera.io created\ncustomresourcedefinition.apiextensions.k8s.io/imagesets.operator.tigera.io created\ncustomresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io created\ncustomresourcedefinition.apiextensions.k8s.io/tigerastatuses.operator.tigera.io created\nserviceaccount/tigera-operator created\nclusterrole.rbac.authorization.k8s.io/tigera-operator created\nclusterrolebinding.rbac.authorization.k8s.io/tigera-operator created\ndeployment.apps/tigera-operator created\n</code></pre></p> <p>Create an installation resource to provision the <code>calico-node</code> daemonset: <pre><code>kubectl create -f - &lt;&lt;EOF\nkind: Installation\napiVersion: operator.tigera.io/v1\nmetadata:\n  name: default\nspec:\n  kubernetesProvider: EKS\n  cni:\n    type: Calico\n  calicoNetwork:\n    bgp: Disabled\nEOF\n</code></pre></p> <p>Expected output:</p> <pre><code>installation.operator.tigera.io/default created\n</code></pre>"},{"location":"tutorials/eks-tutorial/#step-4-provision-nodes","title":"Step 4: Provision nodes","text":"<p>Now, you are ready to provision nodes for the cluster. Use the following description to create two bare-metal nodes, one for each supported architecture (<code>amd64</code> and <code>arm64</code>):</p> <p>Note: Make sure the <code>metadata.name</code> entry corresponds to the name you specified for your cluster above, and that the managedNodeGroups.[].subnets entry correspond to the ones specified above.</p> <pre><code>eksctl create nodegroup -f - &lt;&lt;EOF\n---\napiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\n\nmetadata:\n  name: urunc-tutorial\n  region: eu-central-1\n\nmanagedNodeGroups:\n  - name: a1-metal\n    instanceType: a1.metal\n    amiFamily: Ubuntu2204\n    desiredCapacity: 1\n    minSize: 1\n    maxSize: 1\n    volumeSize: 150\n    volumeType: gp3\n    volumeEncrypted: true\n    privateNetworking: true\n    ssh:\n      allow: true\n      publicKeyName: awseks\n    subnets: [\"subnet-02bcaca5ac39eca7a\",\"subnet-0d0667e2156169998\"]\n    iam:\n      withAddonPolicies:\n        cloudWatch: true\n  - name: c5-metal\n    instanceType: c5.metal\n    amiFamily: Ubuntu2204\n    desiredCapacity: 1\n    minSize: 1\n    maxSize: 1\n    volumeSize: 150\n    volumeType: gp3\n    volumeEncrypted: true\n    privateNetworking: true\n    ssh:\n      allow: true\n      publicKeyName: awseks\n    subnets: [\"subnet-02bcaca5ac39eca7a\",\"subnet-0d0667e2156169998\"]\n    iam:\n      withAddonPolicies:\n        cloudWatch: true\nEOF\n</code></pre> <p>Example output: <pre><code>2025-04-02 12:39:44 [\u2139]  will use version 1.30 for new nodegroup(s) based on control plane version\n2025-04-02 12:39:46 [!]  \"aws-node\" was not found\n2025-04-02 12:39:48 [\u2139]  nodegroup \"a1-metal-cni\" will use \"ami-0eb5f4a5031f47d7b\" [Ubuntu2204/1.30]\n2025-04-02 12:39:49 [\u2139]  using EC2 key pair \"awseks\"\n2025-04-02 12:39:49 [\u2139]  nodegroup \"c5-metal-cni\" will use \"ami-0375252546bcbdbfa\" [Ubuntu2204/1.30]\n2025-04-02 12:39:49 [\u2139]  using EC2 key pair \"awseks\"\n2025-04-02 12:39:50 [\u2139]  2 nodegroups (a1-metal-cni, c5-metal-cni) were included (based on the include/exclude rules)\n2025-04-02 12:39:50 [\u2139]  will create a CloudFormation stack for each of 2 managed nodegroups in cluster \"urunc-tutorial\"\n2025-04-02 12:39:50 [\u2139]\n2 sequential tasks: { fix cluster compatibility, 1 task: {\n2 parallel tasks: { create managed nodegroup \"a1-metal\", create managed nodegroup \"c5-metal\"\n} }\n}\n2025-04-02 12:39:50 [\u2139]  checking cluster stack for missing resources\n2025-04-02 12:39:50 [\u2139]  cluster stack has all required resources\n2025-04-02 12:39:51 [\u2139]  building managed nodegroup stack \"eksctl-urunc-tutorial-nodegroup-a1-metal-cni\"\n2025-04-02 12:39:51 [\u2139]  building managed nodegroup stack \"eksctl-urunc-tutorial-nodegroup-c5-metal-cni\"\n2025-04-02 12:39:51 [\u2139]  deploying stack \"eksctl-urunc-tutorial-nodegroup-c5-metal\"\n2025-04-02 12:39:51 [\u2139]  deploying stack \"eksctl-urunc-tutorial-nodegroup-a1-metal\"\n2025-04-02 12:39:51 [\u2139]  waiting for CloudFormation stack \"eksctl-urunc-tutorial-nodegroup-c5-metal\"\n2025-04-02 12:39:51 [\u2139]  waiting for CloudFormation stack \"eksctl-urunc-tutorial-nodegroup-a1-metal\"\n[...]\n2025-04-02 12:44:05 [\u2139]  no tasks\n2025-04-02 12:44:05 [\u2714]  created 0 nodegroup(s) in cluster \"urunc-tutorial\"\n2025-04-02 12:44:06 [\u2139]  nodegroup \"a1-metal\" has 1 node(s)\n2025-04-02 12:44:06 [\u2139]  node \"ip-192-168-103-211.eu-central-1.compute.internal\" is ready\n2025-04-02 12:44:06 [\u2139]  waiting for at least 1 node(s) to become ready in \"a1-metal\"\n2025-04-02 12:44:06 [\u2139]  nodegroup \"a1-metal\" has 1 node(s)\n2025-04-02 12:44:06 [\u2139]  node \"ip-192-168-103-211.eu-central-1.compute.internal\" is ready\n2025-04-02 12:44:06 [\u2139]  nodegroup \"c5-metal\" has 1 node(s)\n2025-04-02 12:44:06 [\u2139]  node \"ip-192-168-32-137.eu-central-1.compute.internal\" is ready\n2025-04-02 12:44:06 [\u2139]  waiting for at least 1 node(s) to become ready in \"c5-metal\"\n2025-04-02 12:44:06 [\u2139]  nodegroup \"c5-metal\" has 1 node(s)\n2025-04-02 12:44:06 [\u2139]  node \"ip-192-168-32-137.eu-central-1.compute.internal\" is ready\n2025-04-02 12:44:06 [\u2714]  created 2 managed nodegroup(s) in cluster \"urunc-tutorial\"\n2025-04-02 12:44:07 [\u2139]  checking security group configuration for all nodegroups\n2025-04-02 12:44:07 [\u2139]  all nodegroups have up-to-date cloudformation templates\n</code></pre></p>"},{"location":"tutorials/eks-tutorial/#step-5-enable-ssh-access-optional","title":"Step 5: Enable SSH access (optional)","text":"<p>Finally, for debug purposes, enable external SSH access to the nodes:</p> <p>Note: Example for one of the two security groups <pre><code>aws ec2 authorize-security-group-ingress --group-id sg-0d655f9002aec154e --protocol tcp --port 22 --cidr 0.0.0.0/0 --region eu-central-1\n</code></pre></p> <p>Example output: <pre><code>{\n    \"Return\": true,\n    \"SecurityGroupRules\": [\n        {\n            \"SecurityGroupRuleId\": \"sgr-09634d2d1eb260e7a\",\n            \"GroupId\": \"sg-0d655f9002aec154e\",\n            \"GroupOwnerId\": \"058264306458\",\n            \"IsEgress\": false,\n            \"IpProtocol\": \"tcp\",\n            \"FromPort\": 22,\n            \"ToPort\": 22,\n            \"CidrIpv4\": \"0.0.0.0/0\",\n            \"SecurityGroupRuleArn\": \"arn:aws:ec2:eu-central-1:058264306458:security-group-rule/sgr-09634d2d1eb260e7a\"\n        }\n    ]\n}\n</code></pre></p> <p>Below is a script to enable external SSH access to all security groups:</p> <p>Note: Careful, this exposes SSH access to all of your nodes!</p> <pre><code>#!/bin/bash\naws ec2 describe-security-groups --region eu-central-1 --query \"SecurityGroups[*].GroupId\" --output text | tr '\\t' '\\n' | \\\nwhile read sg_id; do\n    echo \"Enabling SSH access for $sg_id...\"\n    aws ec2 authorize-security-group-ingress \\\n        --group-id \"$sg_id\" \\\n        --protocol tcp \\\n        --port 22 \\\n        --cidr 0.0.0.0/0 \\\n        --region eu-central-1 2&gt;&amp;1 | grep -v \"InvalidPermission.Duplicate\"\ndone\n</code></pre>"},{"location":"tutorials/eks-tutorial/#verify-the-cluster-is-operational","title":"Verify the cluster is operational","text":"<p>We have successfully setup the cluster. Let's see what we have using a simple <code>kubectl get pods -o wide -A</code>:</p> <pre><code>NAMESPACE         NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE                                               NOMINATED NODE   READINESS GATES\ncalico-system     calico-kube-controllers-64cf794c44-jnggx   1/1     Running   0          3m52s   172.16.50.196     ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\ncalico-system     calico-node-xcqrj                          1/1     Running   0          3m47s   192.168.32.137    ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\ncalico-system     calico-node-xn6fc                          1/1     Running   0          3m48s   192.168.103.211   ip-192-168-103-211.eu-central-1.compute.internal   &lt;none&gt;           &lt;none&gt;\ncalico-system     calico-typha-84546c84b6-86wfx              1/1     Running   0          3m52s   192.168.32.137    ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\ncalico-system     csi-node-driver-jzs7g                      2/2     Running   0          3m52s   172.16.139.1      ip-192-168-103-211.eu-central-1.compute.internal   &lt;none&gt;           &lt;none&gt;\ncalico-system     csi-node-driver-sstkj                      2/2     Running   0          3m52s   172.16.50.193     ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system       coredns-6f6d89bcc9-dkn6z                   1/1     Running   0          10m     172.16.50.195     ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system       coredns-6f6d89bcc9-ld454                   1/1     Running   0          10m     172.16.50.194     ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\nkube-system       kube-proxy-7mnbs                           1/1     Running   0          4m3s    192.168.103.211   ip-192-168-103-211.eu-central-1.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system       kube-proxy-nx5wk                           1/1     Running   0          4m4s    192.168.32.137    ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\ntigera-operator   tigera-operator-76ff79f7fd-z7t7d           1/1     Running   0          7m17s   192.168.32.137    ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>Also, let's check out the nodes using <code>kubectl get nodes --show-labels</code>:</p> <p><pre><code>$ kubectl get nodes --show-labels\nNAME                                               STATUS   ROLES    AGE   VERSION   LABELS\nip-192-168-103-211.eu-central-1.compute.internal   Ready    &lt;none&gt;   10m   v1.30.6   alpha.eksctl.io/cluster-name=urunc-tutorial,alpha.eksctl.io/instance-id=i-0f1dc1ede23d8e5a7,alpha.eksctl.io/nodegroup-name=a1-metal-cni,beta.kubernetes.io/arch=arm64,beta.kubernetes.io/instance-type=a1.metal,beta.kubernetes.io/os=linux,eks.amazonaws.com/capacityType=ON_DEMAND,eks.amazonaws.com/nodegroup-image=ami-0eb5f4a5031f47d7b,eks.amazonaws.com/nodegroup=a1-metal-cni,eks.amazonaws.com/sourceLaunchTemplateId=lt-0a89f4d0e008cf6f6,eks.amazonaws.com/sourceLaunchTemplateVersion=1,failure-domain.beta.kubernetes.io/region=eu-central-1,failure-domain.beta.kubernetes.io/zone=eu-central-1b,k8s.io/cloud-provider-aws=8c600fe081bc4d4e16d89383ee5c2ac7,kubernetes.io/arch=arm64,kubernetes.io/hostname=ip-192-168-103-211.eu-central-1.compute.internal,kubernetes.io/os=linux,node-lifecycle=on-demand,node.kubernetes.io/instance-type=a1.metal,topology.k8s.aws/zone-id=euc1-az3,topology.kubernetes.io/region=eu-central-1,topology.kubernetes.io/zone=eu-central-1b\nip-192-168-32-137.eu-central-1.compute.internal    Ready    &lt;none&gt;   10m   v1.30.6   alpha.eksctl.io/cluster-name=urunc-tutorial,alpha.eksctl.io/instance-id=i-033fcef7c9cf7b5aa,alpha.eksctl.io/nodegroup-name=c5-metal-cni,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=c5.metal,beta.kubernetes.io/os=linux,eks.amazonaws.com/capacityType=ON_DEMAND,eks.amazonaws.com/nodegroup-image=ami-0375252546bcbdbfa,eks.amazonaws.com/nodegroup=c5-metal-cni,eks.amazonaws.com/sourceLaunchTemplateId=lt-0894d82a5833f577b,eks.amazonaws.com/sourceLaunchTemplateVersion=1,failure-domain.beta.kubernetes.io/region=eu-central-1,failure-domain.beta.kubernetes.io/zone=eu-central-1a,k8s.io/cloud-provider-aws=8c600fe081bc4d4e16d89383ee5c2ac7,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-192-168-32-137.eu-central-1.compute.internal,kubernetes.io/os=linux,node-lifecycle=on-demand,node.kubernetes.io/instance-type=c5.metal,topology.k8s.aws/zone-id=euc1-az2,topology.kubernetes.io/region=eu-central-1,topology.kubernetes.io/zone=eu-central-1a\n</code></pre> Let's do a test deployment. Create a file called <code>nginx-test-deployment.yaml</code> with the following content:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-stock\n  labels:\n    app: nginx-stock\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx-stock\n  template:\n    metadata:\n      labels:\n        app: nginx-stock\n    spec:\n      containers:\n      - name: nginx-stock\n        image: nginx\n        imagePullPolicy: IfNotPresent\n        resources:\n          #limits:\n            #memory: 768Mi\n          requests:\n            memory: 60Mi\n</code></pre> <p>And deploy it: <pre><code>kubectl apply -f nginx-test-deployment.yaml\n</code></pre></p> <p>This should deploy 2 replicas of NGINX. Check the status:</p> <pre><code>kubectl get pods -o wide\n</code></pre> <p>Example output: <pre><code>NAME                           READY   STATUS    RESTARTS   AGE     IP                NODE                                               NOMINATED NODE   READINESS GATES\nnginx-stock-7d54d66484-k9rj5   1/1     Running   0          42s     172.16.50.197     ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\nnginx-stock-7d54d66484-nn696   1/1     Running   0          42s     172.16.139.2      ip-192-168-103-211.eu-central-1.compute.internal   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>And let's try to check network connectivity between pods. Let's run a simple network debug container as a pod:</p> <pre><code>kubectl run tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash\n</code></pre> <p>Expected output: <pre><code>If you don't see a command prompt, try pressing enter.\ntmp-shell:~# \n</code></pre></p> <p>If we issue a simple <code>curl</code> command to one of the pods IPs, we should get a response from the NGINX server: <pre><code>tmp-shell:~# curl 172.16.139.2\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\nhtml { color-scheme: light dark; }\nbody { width: 35em; margin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif; }\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> There we go! We have a working EKS cluster, with Calico and two bare-metal nodes. Time to setup urunc! </p>"},{"location":"tutorials/eks-tutorial/#urunc-setup","title":"<code>urunc</code> setup","text":"<p>The easiest way to setup <code>urunc</code> in such a setting is to use <code>urunc-deploy</code>. This process follows the principles of <code>kata-deploy</code> and is build to work on <code>k8s</code> and <code>k3s</code>. The process is as follows:</p>"},{"location":"tutorials/eks-tutorial/#1-clone-the-repo","title":"1. Clone the repo","text":"<pre><code>git clone https://github.com/urunc-dev/urunc\n</code></pre>"},{"location":"tutorials/eks-tutorial/#2-apply-the-manifests","title":"2.  Apply the manifests","text":"<p>First we need to create the RBAC <pre><code>kubectl apply -f deployment/urunc-deploy/urunc-rbac/urunc-rbac.yaml\n</code></pre></p> <p>Then, we create the <code>urunc-deploy</code> daemonset: <pre><code>kubectl apply -f deployment/urunc-deploy/urunc-deploy/base/urunc-deploy.yaml\n</code></pre></p> <p>Finally, we need to create the appropriate k8s runtime class: <pre><code>kubectl apply -f deployment/urunc-deploy/runtimeclasses/runtimeclass.yaml\n</code></pre></p> <p>Example output:</p> <pre><code>serviceaccount/urunc-deploy-sa created\nclusterrole.rbac.authorization.k8s.io/urunc-deploy-role created\nclusterrolebinding.rbac.authorization.k8s.io/urunc-deploy-rb created\ndaemonset.apps/urunc-deploy created\nruntimeclass.node.k8s.io/urunc created\n</code></pre> <p>Monitor the deploy pods once they change their status to <code>Running</code>: <pre><code>kubectl logs -f -n kube-system -l name=urunc-deploy\n</code></pre></p> <p>Example output: <pre><code>Installing qemu\nInstalling solo5-hvt\nInstalling solo5-spt\nAdd urunc as a supported runtime for containerd\nContainerd conf file: /etc/containerd/config.toml\nPlugin ID: \"io.containerd.grpc.v1.cri\"\nOnce again, configuration file is /etc/containerd/config.toml\nreloading containerd\nnode/ip-192-168-103-211.eu-central-1.compute.internal labeled\nurunc-deploy completed successfully\nInstalling qemu\nInstalling solo5-hvt\nInstalling solo5-spt\nAdd urunc as a supported runtime for containerd\nContainerd conf file: /etc/containerd/config.toml\nPlugin ID: \"io.containerd.grpc.v1.cri\"\nOnce again, configuration file is /etc/containerd/config.toml\nreloading containerd\nnode/ip-192-168-32-137.eu-central-1.compute.internal labeled\nurunc-deploy completed successfully\n</code></pre></p> <p>Now we've got urunc installed on each node, along with the supported hypervisors! Let's try to deploy a unikernel! </p>"},{"location":"tutorials/eks-tutorial/#run-a-unikernel","title":"Run a unikernel","text":"<p>Create a YAML file (e.g. <code>nginx-urunc.yaml</code>) with the following contents:</p> <p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: nginx-urunc\n  name: nginx-urunc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: nginx-urunc\n  template:\n    metadata:\n      labels:\n        run: nginx-urunc\n    spec:\n      runtimeClassName: urunc\n      containers:\n      - image: harbor.nbfc.io/nubificus/urunc/nginx-qemu-unikraft-initrd:latest\n        imagePullPolicy: Always\n        name: nginx-urunc\n        command: [\"sleep\"]\n        args: [\"infinity\"]\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 10m\n      restartPolicy: Always\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-urunc\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    run: nginx-urunc\n  sessionAffinity: None\n  type: ClusterIP\n</code></pre> Issuing the command below: <pre><code>kubectl apply -f nginx-urunc.yaml\n</code></pre> will produce the following output: <pre><code>deployment.apps/nginx-urunc created\nservice/nginx-urunc created\n</code></pre> and will create a deployment of an NGINX unikernel, from the container image pushed at <code>harbor.nbfc.io/nubificus/urunc/nginx-hvt-rumprun-block:latest</code></p> <p>Inspecting the pods with <code>kubectl get pods -o wide</code> reveals the status: <pre><code>default           nginx-urunc-998b889c4-x798f                1/1     Running             0          2s      172.16.50.225     ip-192-168-32-137.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>and following up on the previous test, we do:</p> <p><pre><code>kubectl run tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash\n</code></pre> To get a shell in a pod in the cluster: <pre><code>If you don't see a command prompt, try pressing enter.\ntmp-shell:~# \n</code></pre></p> <p>and we <code>curl</code> the pod's IP:</p> <pre><code>tmp-shell:~# curl 172.16.50.225\n&lt;html&gt;\n&lt;body style=\"font-size: 14pt;\"&gt;\n    &lt;img src=\"logo150.png\"/&gt;\n    Served to you by &lt;a href=\"http://nginx.org/\"&gt;nginx&lt;/a&gt;, running on a\n    &lt;a href=\"http://rumpkernel.org\"&gt;rump kernel&lt;/a&gt;...\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"tutorials/eks-tutorial/#conclusions","title":"Conclusions","text":"<p>You now have a fully functional EKS cluster with custom VPC networking and Calico CNI, all set up to run unikernel containers via <code>urunc</code>.</p> <p>We have covered how to:</p> <ul> <li>Provision foundational infrastructure on AWS  </li> <li>Deploy a secure and customizable Kubernetes cluster   </li> <li>Configure networking via Calico  </li> <li>Prepare node groups with SSH access for hands-on debugging or remote setup</li> <li>Install <code>urunc</code> via <code>urunc-deploy</code></li> <li>Deploy an example unikernel</li> </ul> <p>With your EKS cluster up and running, equipped with Calico networking and ready for <code>urunc</code>, you now have a powerful, Kubernetes-native foundation for exploring the next generation of lightweight, secure container runtimes!</p>"},{"location":"tutorials/existing-container-linux/","title":"Running existing containers in <code>urunc</code> with Linux","text":"<p>While Linux is not a unikernel framework, it remains the most widely used kernel in cloud infrastructure. As a result, the majority of applications and services are built to run on Linux. At the same time, Linux has a very highly configurable build system, and as proven by Lupine, we can build tailored Linux kernels optimized for running a single application.</p> <p>With this goal in mind, this guide walks through the steps required to take an existing container image and execute it on top of <code>urunc</code> as a Linux virtual machine (VM).</p> <p>Overall, we need to do the followings:</p> <ol> <li>Build or reuse a Linux kernel.</li> <li>(Optional) Build or fetch an init process.</li> <li>Prepare the final image by appending the Linux kernel (and init) and set up    <code>urunc</code> annotations.</li> </ol>"},{"location":"tutorials/existing-container-linux/#linux-kernel","title":"Linux kernel","text":"<p>The main requirement for running existing containers on top of <code>urunc</code> is a Linux kernel. From <code>urunc</code>'s side there are no specific kernel configuration options required, but since Linux will run on virtual machine monitors like Qemu or Firecracker, the kernel should be configured with the necessary drivers (e.g., virtio devices).</p> <p>To simplify this, you can find here a sample x86 kernel configuration based on Linux v6.14, which builds a minimal kernel around 13\u202fMiB in size. Note that this configuration excludes features like cgroups and certain system calls, so additional customization may be required depending on your application.</p> <p>Alternatively, prebuilt kernels are available via the following container images:</p> <ul> <li><code>harbor.nbfc.io/nubificus/urunc/linux-kernel-qemu:v6.14</code></li> <li><code>harbor.nbfc.io/nubificus/urunc/linux-kernel-firecracker:v6.14</code></li> </ul> <p>Each image contains the Linux kernel binary at <code>/kernel</code>.</p>"},{"location":"tutorials/existing-container-linux/#init-process","title":"Init process","text":"<p>After booting, the Linux kernel hands control to the init process, the first user-space program. This process acts as the root of the process tree and must remain running. If it exits, the kernel will panic.</p> <p>In single-application environments, the application itself can serve as init. However, this is not always reliable:</p> <ul> <li>If the application exits, the system halts.</li> <li>CLI argument handling may be incorrect: Linux does not natively support   multi-word arguments via kernel boot parameters. Each space-separated word is   treated as a separate argument.</li> </ul> <p>To tackle this, <code>urunc</code> follows a simple convention. All multi-word CLI arguments are wrapped in single quotes and the init process (or application) is expected to reconstruct them properly.</p> <p>For these reasons, we recommend introducing a dedicated init process. We provide urunit; a lightweight init designed specifically for <code>urunc</code>. It performs two key roles:</p> <ol> <li>Groups multi-word arguments correctly.</li> <li>Acts as a reaper, cleaning up zombie processes.</li> </ol> <p>You can obtain urunit in two ways:</p> <ul> <li>Fetch a static binary from urunit's release   page.   Via the container image: <code>harbor.nbfc.io/nubificus/urunit:latest</code>,   with the binary located at <code>/urunit</code>.</li> </ul>"},{"location":"tutorials/existing-container-linux/#preparing-the-image","title":"Preparing the image","text":"<p>To differentiate traditional containers from unikernels, <code>urunc</code> uses specific annotations. Therefore, to run a container with a Linux kernel on <code>urunc</code>, these annotations must be configured, and the Linux kernel must be included in the container image\u2019s root filesystem. To simplify this process, we will use bunny.</p> <p>Another important aspect is preparing the root filesystem (rootfs). Since we're booting a full Linux virtual machine, a proper rootfs must be provided. There are three main ways to do this:</p> <ol> <li>Using directly the rootfs of the container's image (requires devmapper).</li> <li>Creating a block image out of a container's image rootfs.</li> <li>Creating a initrd.</li> </ol>"},{"location":"tutorials/existing-container-linux/#using-directly-the-containers-rootfs","title":"Using directly the container's rootfs","text":"<p>The simplest way to boot an existing container with a Linux kernel on <code>urunc</code> is to reuse the container\u2019s rootfs. However, since <code>urunc</code> does not yet support shared filesystems between host and guest, this method currently requires using devmapper as the snapshotter.  In that way, containerd's devmapper snapshotter will create a block image out of the container's rootfs and <code>urunc</code> can easily attach this block image to the VM.</p> <p>To set up devmapper as a snapshotter please refer to the installation guide.</p>"},{"location":"tutorials/existing-container-linux/#preparing-the-container-image","title":"Preparing the container image.","text":"<p>In this case preparing the container image involves two key steps:</p> <ol> <li>Append the Linux kernel binary to the container image.</li> <li>Set the appropriate <code>urunc</code> annotations.</li> </ol> <p>These tasks can be easily automated with bunny.</p> <p>Let's use as an example the <code>redis:alpine</code> container image using the Linux kernel from <code>harbor.nbfc.io/nubificus/urunc/linux-kernel-qemu:v6.14</code>. The respective <code>bunnyfile</code> would look like:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: linux\n  monitor: qemu\n  architecture: x86\n\nrootfs:\n  from: redis:alpine\n  type: raw\n\nkernel:\n  from: harbor.nbfc.io/nubificus/urunc/linux-kernel-qemu:v6.14\n  path: /kernel\n\ncmdline: \"/usr/local/bin/redis-server\"\n</code></pre> <p>We can build the container with:</p> <pre><code>docker build -f bunnyfile -t redis/apline/linux/qemu:latest .\n</code></pre> <p>Alternatively, if the Linux kernel was built locally, we can update the kernel section of the bunnyfile to reference the local binary:</p> <pre><code>kernel:\n  from: local\n  path: bzImage\n</code></pre> <p>By default, this setup will run redis-server as the init process.  To include urunit in the redis:alpine image, we can use the following Containerfile:</p> <pre><code>FROM harbor.nbfc.io/nubificus/urunit:latest AS init\n\nFROM redis:alpine\n\nCOPY --from=init /urunit /urunit\n</code></pre> <p>NOTE: We are working towards enabling the addition of extra files from the <code>bunnyfile</code>. We will update this page once this feature is supported.</p> <p>After building the above container, make sure to specify it in the <code>from</code> field of rootfs in <code>bunnyfile</code>:</p> <pre><code>rootfs:\n  from: redis/urunit:alpine\n  type: raw\n</code></pre> <p>At last we need to modify the <code>cmdline</code> section of <code>bunnyfile</code> to execute urunit:</p> <pre><code>cmdline: \"/urunit /usr/local/bin/redis-server\"\n</code></pre>"},{"location":"tutorials/existing-container-linux/#running-the-container","title":"Running the container","text":"<p>Unfortunately, Docker requires additional setup to work with the devmapper snapshotter. To bypass this limitation, we will use nerdctl, which integrates seamlessly with containerd and supports devmapper out of the box.</p> <p>First, transfer the container image from Docker\u2019s image store to containerd: <pre><code>docker save redis/apline/linux/qemu:latest | nerdctl load\n</code></pre></p> <p>With the image now available in containerd, we\u2019re ready to run the container using urunc and the devmapper snapshotter:</p> <pre><code>nerdctl run --rm -it --snapshotter devmapper --runtime \"io.containerd.urunc.v2\" redis/apline/linux/qemu:latest\n</code></pre> <p>Let's find the IP of the container: <pre><code>$ nerdctl inspect &lt;CONTAINER ID&gt; | grep IPAddress\n            \"IPAddress\": \"10.4.0.2\",\n                    \"IPAddress\": \"10.4.0.2\",\n                    \"IPAddress\": \"172.16.1.2\",\n</code></pre></p> <p>and we should be able to ping it:</p> <pre><code>ping -c 3 10.4.0.2\n</code></pre>"},{"location":"tutorials/existing-container-linux/#using-a-block-image","title":"Using a block image","text":"<p>If we are not able to set up devmapper or we have a block image that can be used as a rootfs, we can instruct <code>urunc</code> to use a block image.</p>"},{"location":"tutorials/existing-container-linux/#preparing-the-container-image_1","title":"Preparing the container image.","text":"<p>To prepare the container image we will need to first create block image. For that purpose, we will use <code>nginx:alpine</code> image and we will choose to run it on top of Firecracker. We can create the block image with the following steps:</p> <pre><code>dd if=/dev/zero of=rootfs.ext2 bs=1 count=0 seek=60M\nmkfs.ext2 rootfs.ext2\nmkdir tmp_mnt\nmount rootfs.ext2 tmp_mnt\ndocker export $(docker create nginx:alpine) -o nginx_alpine.tar\ntar -xf nginx_alpine.tar -C tmp_mnt\nwget -O tmp_mnt/urunit https://github.com/nubificus/urunit/releases/download/v0.1.0/urunit_x86_64 # If we want urunit as init\nchmod +x tmp_mnt/urunit # If we want urunit as init\numount tmp_mnt\n</code></pre> <p>Now we have a block image, <code>rootfs.ext2</code>, generated from the <code>nginx:alpine</code> container and including urunit latest release. To package everything together, we will use a file with Containerfile-like syntax, just to demonstrate how to manually define the required annotations for <code>urunc</code>:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nFROM scratch\n\nCOPY vmlinux /kernel\nCOPY nginx_rootfs.ext2 /rootfs.ext2\n\nLABEL \"com.urunc.unikernel.binary\"=\"/kernel\"\nLABEL \"com.urunc.unikernel.cmdline\"=\"/urunit /usr/sbin/nginx -g 'daemon off;error_log stderr debug;\"\nLABEL \"com.urunc.unikernel.unikernelType\"=\"linux\"\nLABEL \"com.urunc.unikernel.block\"=\"/rootfs.ext2\"\nLABEL \"com.urunc.unikernel.blkMntPoint\"=\"/\"\nLABEL \"com.urunc.unikernel.hypervisor\"=\"firecracker\"\n</code></pre> <p>We can build the container with:</p> <pre><code>docker build -f Containerfile -t nginx/apline/linux/firecracker:latest .\n</code></pre>"},{"location":"tutorials/existing-container-linux/#running-the-container_1","title":"Running the container","text":"<p>In this case, we can directly use docker to run the container, since there is no need for devmapper.</p> <pre><code>docker run --rm -it --runtime \"io.containerd.urunc.v2\" nginx/apline/linux/firecracker:latest\n</code></pre> <p>Let's find the IP of the container: <pre><code>$ docker inspect &lt;CONTAINER ID&gt; | grep IPAddress\n            \"SecondaryIPAddresses\": null,\n            \"IPAddress\": \"172.17.0.2\",\n                    \"IPAddress\": \"172.17.0.2\",\n</code></pre></p> <p>and we should be able to curl it:</p> <pre><code>$ curl 172.17.0.2\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\nhtml { color-scheme: light dark; }\nbody { width: 35em; margin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif; }\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"tutorials/existing-container-linux/#using-initrd-as-a-rootfs","title":"Using initrd as a rootfs","text":"<p>Similarly to the previous approach, we can create an initrd instead of a block image to use as the root filesystem. To demonstrate this, we will use the <code>traefik/whoami</code> container image as an example.</p>"},{"location":"tutorials/existing-container-linux/#preparing-the-container-image_2","title":"Preparing the container image.","text":"<p>First let's create the initrd:</p> <pre><code>mkdir tmp_rootfs\ndocker export $(docker create traefik/whoami) | tar -C tmp_rootfs/ -xvf -\nwget -O tmp_rootfs/urunit https://github.com/nubificus/urunit/releases/download/v0.1.0/urunit_x86_64 # If we want urunit as init\nchmod +x tmp_rootfs/urunit\ncd tmp_rootfs\nfind . | cpio -H newc -o &gt; ../rootfs.initrd\n</code></pre> <p>NOTE: We are working towards enabling the creation of the initrd directly from bunny. We will update this page once this feature is supported.</p> <p>Now we have an initrd <code>rootfs.initrd</code> generated from <code>traefik/whoami</code> and with urunit that we got from its latest release.  In order to pack everything together, we can use the following <code>bunnyfile</code>:</p> <pre><code>#syntax=harbor.nbfc.io/nubificus/bunny:latest\nversion: v0.1\n\nplatforms:\n  framework: linux\n  monitor: firecracker\n  architecture: x86\n\nrootfs:\n  from: local\n  type: initrd\n  path: rootfs.initrd\n\nkernel:\n  from: harbor.nbfc.io/nubificus/urunc/linux-kernel-firecracker:v6.14\n  path: /kernel\n\ncmdline: \"/urunit /whoami\"\n</code></pre> <p>We can build the container with:</p> <pre><code>docker build -f bunnyfile -t traefik/whoami/linux/firecracker:latest .\n</code></pre>"},{"location":"tutorials/existing-container-linux/#running-the-container_2","title":"Running the container","text":"<p>In this case, we can directly use docker to run the container, since there is no need for devmapper.</p> <pre><code>docker run --rm -it --runtime \"io.containerd.urunc.v2\" traefik/whoami/linux/firecracker:latest\n</code></pre> <p>Let's find the IP of the container: <pre><code>$ docker inspect &lt;CONTAINER ID&gt; | grep IPAddress\n            \"SecondaryIPAddresses\": null,\n            \"IPAddress\": \"172.17.0.2\",\n                    \"IPAddress\": \"172.17.0.2\",\n</code></pre></p> <p>and we should be able to curl it:</p> <pre><code>$ curl 172.17.0.2\nHostname: urunc\nIP: 127.0.0.1\nIP: 172.17.0.2\nRemoteAddr: 172.17.0.1:42684\nGET / HTTP/1.1\nHost: 172.17.0.2\nUser-Agent: curl/7.68.0\nAccept: */*\n</code></pre>"},{"location":"tutorials/knative/","title":"Knative + urunc: Deploying Serverless Unikernels","text":"<p>This guide walks you through deploying Knative Serving using <code>urunc</code>. You\u2019ll build Knative from a custom branch and use <code>ko</code> for seamless image building and deployment.</p>"},{"location":"tutorials/knative/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running Kubernetes cluster</li> <li>A Docker-compatible registry (e.g. Harbor, Docker Hub)</li> <li>Ubuntu 20.04 or newer</li> <li>Basic <code>git</code>, <code>curl</code>, <code>kubectl</code>, and <code>docker</code> installed</li> </ul>"},{"location":"tutorials/knative/#environment-setup","title":"Environment Setup","text":"<p>Install Docker, Go &gt;= 1.21, and <code>ko</code>:</p>"},{"location":"tutorials/knative/#install-go-121","title":"Install Go 1.21","text":"<pre><code>sudo mkdir /usr/local/go1.21\nwget https://go.dev/dl/go1.21.5.linux-amd64.tar.gz\nsudo tar -zxvf go1.21.5.linux-amd64.tar.gz -C /usr/local/go1.21/\nrm go1.21.5.linux-amd64.tar.gz\n</code></pre>"},{"location":"tutorials/knative/#verify-go-installation-should-be-1215","title":"Verify Go installation (Should be 1.21.5)","text":"<pre><code>$ export GOROOT=/usr/local/go1.21/go \n$ export PATH=$GOROOT/bin:$PATH  \n$ export GOPATH=$HOME/go \n$ go version\ngo version go1.21.5 linux/amd64\n</code></pre>"},{"location":"tutorials/knative/#install-ko-version0151","title":"Install ko VERSION=0.15.1","text":"<pre><code>export OS=Linux\nexport ARCH=x86_64\ncurl -sSfL \"https://github.com/ko-build/ko/releases/download/v${VERSION}/ko_${VERSION}_${OS}_${ARCH}.tar.gz\" -o ko.tar.gz\nsudo tar -zxvf ko.tar.gz -C /usr/local/bin` \n</code></pre>"},{"location":"tutorials/knative/#clone-and-build-knative-with-the-queue-proxy-patch","title":"Clone and Build Knative with the queue-proxy patch","text":""},{"location":"tutorials/knative/#set-your-container-registry","title":"Set your container registry","text":"<p>Note: You should be able to use dockerhub for this. e.g. <code>&lt;yourdockerhubid&gt;/knative</code></p> <pre><code>export KO_DOCKER_REPO='harbor.nbfc.io/nubificus/knative-install-urunc'\n</code></pre>"},{"location":"tutorials/knative/#clone-urunc-enabled-knative-serving","title":"Clone urunc-enabled Knative Serving","text":"<pre><code>git clone https://github.com/nubificus/serving -b feat_urunc \ncd serving/\nko resolve -Rf ./config/core/ &gt; knative-custom.yaml\n</code></pre>"},{"location":"tutorials/knative/#apply-knatives-manifests-to-the-local-k8s","title":"Apply knative's manifests to the local k8s","text":"<pre><code>kubectl apply -f knative-custom.yaml\n</code></pre> <p>Alternatively, you could use our latest build: <pre><code>kubectl apply -f https://s3.nbfc.io/knative/knative-v1.17.0-urunc-5220308.yaml\n</code></pre></p> <p>Note: There are cases where due to the large manifests, kubectl fails. Try a second time, or use <code>kubectl create -f https://s3.nbfc.io/knative/knative-v1.17.0-urunc-5220308.yaml</code></p>"},{"location":"tutorials/knative/#setup-networking-kourier","title":"Setup Networking (Kourier)","text":""},{"location":"tutorials/knative/#install-kourier-patch-ingress-and-domain-configs","title":"Install kourier, patch ingress and domain configs","text":"<pre><code>kubectl apply -f https://github.com/knative/net-kourier/releases/latest/download/kourier.yaml \nkubectl patch configmap/config-network -n knative-serving --type merge -p \\ \n  '{\"data\":{\"ingress.class\":\"kourier.ingress.networking.knative.dev\"}}'\nkubectl patch configmap/config-domain -n knative-serving --type merge -p \\ \n  '{\"data\":{\"127.0.0.1.nip.io\":\"\"}}'\n</code></pre>"},{"location":"tutorials/knative/#enable-runtimeclass-and-urunc-support","title":"Enable RuntimeClass and urunc Support","text":""},{"location":"tutorials/knative/#install-urunc","title":"Install <code>urunc</code>","text":"<p>You can follow the documentation to install <code>urunc</code> from: Installing</p>"},{"location":"tutorials/knative/#enable-runtimeclass-for-services-nodeselector-and-affinity","title":"Enable runtimeClass for services, nodeSelector and affinity","text":"<pre><code>kubectl patch configmap/config-features --namespace knative-serving --type merge --patch '{\"data\":{\n  \"kubernetes.podspec-affinity\":\"enabled\",\n  \"kubernetes.podspec-runtimeclassname\":\"enabled\",\n  \"kubernetes.podspec-nodeselector\":\"enabled\"\n}}'\n</code></pre>"},{"location":"tutorials/knative/#deploy-a-sample-urunc-service","title":"Deploy a Sample urunc Service","text":"<pre><code>kubectl get ksvc -A -o wide\n</code></pre> <p>Should be empty. Create an simple httpreply service, based on a simple C program:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/nubificus/c-httpreply/refs/heads/main/service.yaml\n</code></pre>"},{"location":"tutorials/knative/#check-knative-service","title":"Check Knative Service","text":"<pre><code>kubectl get ksvc -A -o wide \n</code></pre>"},{"location":"tutorials/knative/#test-the-service-replace-ip-with-actual-ingress-ip","title":"Test the service (replace IP with actual ingress IP)","text":"<pre><code>curl -v -H \"Host: hellocontainerc.default.127.0.0.1.nip.io\" http://&lt;INGRESS_IP&gt;\n</code></pre> <p>Now, let's create a <code>urunc</code>-compatible function. Create a service, based on Unikraft's httreply example: </p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/nubificus/app-httpreply/refs/heads/feat_generic/service.yaml\n</code></pre> <p>You should be able to see this being created:</p> <pre><code>$ kubectl get ksvc -o wide\nNAME             URL                                                  LATESTCREATED              LATESTREADY                READY   REASON\nhellounikernelfc http://hellounikernelfc.default.127.0.0.1.nip.io     hellounikernelfc-00001     hellounikernelfc-00001     True\n</code></pre> <p>and once it's on a <code>Ready</code> state, you could issue a request:</p> <p>Note: 10.244.9.220 is the IP of the <code>kourier-internal</code> svc. You can check your own from: <code>kubectl get svc -n kourier-system |grep kourier-internal</code></p> <pre><code>$ curl -v -H \"Host: hellounikernelfc.default.127.0.0.1.nip.io\" http://10.244.9.220:80\n*   Trying 10.244.9.220:80...\n* Connected to 10.244.9.220 (10.244.9.220) port 80 (#0)\n&gt; GET / HTTP/1.1\n&gt; Host: hellounikernelfc.default.127.0.0.1.nip.io\n&gt; User-Agent: curl/7.81.0\n&gt; Accept: */*\n&gt;\n* Mark bundle as not supporting multiuse\n&lt; HTTP/1.1 200 OK\n&lt; content-length: 14\n&lt; content-type: text/html; charset=UTF-8\n&lt; date: Tue, 08 Apr 2025 15:47:45 GMT\n&lt; x-envoy-upstream-service-time: 774\n&lt; server: envoy\n&lt;\nHello, World!\n* Connection #0 to host 10.244.9.220 left intact\n</code></pre>"},{"location":"tutorials/knative/#wrapping-up","title":"Wrapping Up","text":"<p>You're now running unikernel-based workloads via Knative and <code>urunc</code>! With this setup, you can push the boundaries of lightweight, secure, and high-performance serverless deployments \u2014 all within a Kubernetes-native environment.</p>"}]}